{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ERgBpbcMmB"
      },
      "source": [
        "# HW1: Frame-Level Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkH6GMGcWcE"
      },
      "source": [
        "In this homework, you will be working with MFCC data consisting of 27 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rwYu9sSUnSho"
      },
      "outputs": [],
      "source": [
        "# !pip install torchsummaryX wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "ef79a3fc-5689-4e5a-d896-329b8a9d6a5c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/zzy/miniconda3/envs/11785/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8yBgXjKV1O0Z"
      },
      "outputs": [],
      "source": [
        "### If you are using colab, you can import google drive to save model checkpoints in a folder\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N-9qE20hmCgQ"
      },
      "outputs": [],
      "source": [
        "### PHONEME LIST\n",
        "PHONEMES = [\n",
        "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configs:\n",
        "DATA_ROOT = \"/mnt/e/Workspace/IDL/Data/hw1/11-785-s24-hw1p2/\"\n",
        "MODEL_ROOT = \"/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBCbeRhixGM7"
      },
      "source": [
        "This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TPBUd7Cnl-Rx"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "# !mkdir /root/.kaggle\n",
        "\n",
        "# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "#     f.write('{\"username\":\"Replace this with your Kaggle Username\",\"key\":\"Replace this with your kaggle API key\"}')\n",
        "#     # Put your kaggle username & key here\n",
        "\n",
        "# !chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "if2Somqfbje1"
      },
      "outputs": [],
      "source": [
        "# commands to download data from kaggle\n",
        "# !kaggle competitions download -c 11785-hw1p2-s24\n",
        "\n",
        "# !unzip -qo /content/11785-hw1p2-s24.zip -d '/content'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuzce0_TdcaR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7QgMbBdgPp"
      },
      "source": [
        "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
        "\n",
        "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YpLCvi3AJC5z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2703/2703 [00:39<00:00, 67.98it/s]\n"
          ]
        }
      ],
      "source": [
        "# Dataset class to load train and validation data\n",
        "from torch.nn.functional import one_hot\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#TODO: dynamic padding\n",
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, root=DATA_ROOT, phonemes = PHONEMES, context=0, partition= \"train-clean-100\", use_cmn=False): #TODO: make no-pad works (BF2042 meme huh?) \n",
        "        self.max_context_length = 1145 #Magic number\n",
        "        self.set_context_length(context)\n",
        "        self.phonemes   = phonemes\n",
        "        \n",
        "        self.num_phonemes = len(self.phonemes)\n",
        "        \n",
        "        self.mfccs, self.transcripts = self._init_data(f\"{root}/{partition}\", use_cmn=use_cmn)\n",
        "        self.length = len(self.mfccs)\n",
        "        \n",
        "        self.mfccs = np.concatenate([np.zeros((self.max_context_length, 27)), self.mfccs, np.zeros((self.max_context_length, 27))], axis=0)\n",
        "        \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # given current context length, compute offset:\n",
        "        lower = idx + self.max_context_length - self.context\n",
        "        upper = idx + self.max_context_length + self.context + 1\n",
        "            \n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[lower:upper]\n",
        "        \n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten() # Reshape to 1d array\n",
        "\n",
        "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
        "        phonemes    = torch.tensor(self.transcripts[idx]) # Get the phoneme at the index\n",
        "\n",
        "        return frames, phonemes\n",
        "        \n",
        "        \n",
        "    def _init_data(self, root: str, use_cmn = False):\n",
        "        self.mfcc_dir       = f\"{root}/mfcc\"\n",
        "        self.transcript_dir = f\"{root}/transcript\"\n",
        "        mfcc_names          = os.listdir(self.mfcc_dir)\n",
        "        transcript_names    = os.listdir(self.transcript_dir)\n",
        "        \n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        for i in tqdm(range(len(mfcc_names))):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            if use_cmn:\n",
        "                mfcc = mfcc - np.mean(mfcc, axis=0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = np.load(f\"{self.transcript_dir}/{transcript_names[i]}\") \n",
        "            # Remove [SOS] and [EOS] from the transcript\n",
        "            assert transcript[0] == '[SOS]' and transcript[-1] == '[EOS]'\n",
        "            transcript = transcript[1:-1]\n",
        "            #lookup phoneme index\n",
        "            transcript = np.vectorize(self.phonemes.index)(transcript)\n",
        "            assert len(mfcc) == len(transcript)\n",
        "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
        "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "            \n",
        "        return np.concatenate(self.mfccs, axis=0), np.concatenate(self.transcripts, axis=0)\n",
        "    \n",
        "    def set_context_length(self, context):\n",
        "        self.context = context\n",
        "        \n",
        "    def phoneme_reverse_lookup(self, idx: torch.tensor) -> str:\n",
        "        return self.phonemes[idx]\n",
        "         \n",
        "         \n",
        "val_data = AudioDataset(partition=\"dev-clean\", context=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([297]) tensor(3) AH\n"
          ]
        }
      ],
      "source": [
        "f, p = val_data[114514]\n",
        "print(f.shape, p, val_data.phoneme_reverse_lookup(p))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e8KfVP39S6o7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2620/2620 [00:20<00:00, 130.36it/s]\n"
          ]
        }
      ],
      "source": [
        "from numpy import ndarray\n",
        "\n",
        "\n",
        "class AudioTestDataset(AudioDataset):\n",
        "    \n",
        "    def _init_data(self, root: str, use_cmn):\n",
        "        \n",
        "        self.mfcc_dir = f\"{root}/mfcc\"\n",
        "\n",
        "        mfcc_names = os.listdir(self.mfcc_dir)\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "        \n",
        "        for i in tqdm(range(len(mfcc_names))):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
        "            transcript = np.array([0 for _ in range(len(mfcc))])\n",
        "            \n",
        "            assert len(mfcc) == len(transcript)\n",
        "            \n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "            \n",
        "        return np.concatenate(self.mfccs, axis=0), np.concatenate(self.transcripts, axis=0)\n",
        "    \n",
        "    def __getitem__(self, ind):\n",
        "        return super().__getitem__(ind)[0]\n",
        "    \n",
        "\n",
        "test_data = AudioTestDataset(partition=\"test-clean\", context=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([297])\n"
          ]
        }
      ],
      "source": [
        "f = test_data[114514]\n",
        "print(f.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28539/28539 [07:11<00:00, 66.12it/s]\n"
          ]
        }
      ],
      "source": [
        "train_data = AudioDataset(partition=\"train-clean-100\", context=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This architecture will make you cross the very low cutoff\n",
        "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
        "class LowCutoffNet(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "\n",
        "        super(LowCutoffNet, self).__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 256),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(256, 2048),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(0.2),\n",
        "            torch.nn.Linear(2048, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn.modules import Module\n",
        "\n",
        "class DynamicMlpNet(torch.nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_sizes, dropout_rate):\n",
        "        super(DynamicMlpNet, self).__init__()\n",
        "        self.layers = []\n",
        "        for i, hs in enumerate(hidden_sizes):\n",
        "            self.layers.extend(self._mlp_layer_provider(input_size, hs, dropout_rate))\n",
        "            input_size = hs\n",
        "        self.layers.append(nn.Linear(input_size, output_size)) # output\n",
        "        self.model = nn.Sequential(*self.layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "        return out\n",
        "    \n",
        "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate) -> list[nn.Module]:\n",
        "        return [\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ]\n",
        "    \n",
        "    def describe() -> str:\n",
        "        return \"base model, don't use this\"\n",
        "        \n",
        "class NetV2(DynamicMlpNet):\n",
        "    def __init__(self, input_size, output_size, dropout_rate, hidden_sizes=np.array([1,2,4,2,1]) * 512):\n",
        "        return super().__init__(input_size, output_size, hidden_sizes, dropout_rate)\n",
        "    \n",
        "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate) -> list[Module]:\n",
        "        return [\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Mish(),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ]\n",
        "    \n",
        "    def describe() -> str:\n",
        "        return \"5layer_mlp_v2\"\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Param size 23M\n",
        "class NetV3(DynamicMlpNet):\n",
        "    def __init__(self, input_size, output_size, dropout_rate, hidden_sizes=np.array([1,2,4,8,4,2,1]) * 512):\n",
        "        return super().__init__(input_size, output_size, hidden_sizes, dropout_rate)\n",
        "    \n",
        "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate) -> list[Module]:\n",
        "        return [\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Mish(),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ]\n",
        "    \n",
        "    def describe() -> str:\n",
        "        return \"7layer_mlp_v3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Non-symmetric shape, param 21M\n",
        "class NetV4(NetV3):\n",
        "    def __init__(self, input_size, output_size, dropout_rate, hidden_sizes=np.array([8,16,4,1,4,8,4,2]) * 256):\n",
        "        return super().__init__(input_size, output_size, dropout_rate, hidden_sizes) \n",
        "    \n",
        "    def describe() -> str:\n",
        "        return \"8layer_mlp_asym_v3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DynamicDenseMlp(nn.Module):\n",
        "    def __init__(self, input_size, output_size, dropout_rate, hidden_sizes):\n",
        "        super(DynamicDenseMlp, self).__init__()\n",
        "        feedforward_size = 0\n",
        "        self.dense_layers = []\n",
        "        for i, hidden_size in enumerate(hidden_sizes):\n",
        "            self.dense_layers.append(nn.Sequential(*self._mlp_layer_provider(input_size + feedforward_size, hidden_size, dropout_rate)))\n",
        "            # Don't skip input:\n",
        "            if i > 0:\n",
        "                feedforward_size += input_size\n",
        "            input_size = hidden_size\n",
        "        self.dense_layers.append(nn.Linear(input_size + feedforward_size, output_size))\n",
        "        self.model = nn.Sequential(*self.dense_layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        current_layer_input = x\n",
        "        for i, layer in enumerate(self.dense_layers):\n",
        "            out = layer(current_layer_input)\n",
        "            if i > 0: # Don't skip input\n",
        "                current_layer_input = torch.cat((current_layer_input, out), dim=1)\n",
        "            else:\n",
        "                current_layer_input = out\n",
        "        return out    \n",
        "\n",
        "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate) -> list[Module]:\n",
        "        return [\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "        ]\n",
        "\n",
        "    def describe() -> str:\n",
        "        return \"dense_mlp_base\"    \n",
        "\n",
        "#param 21M    \n",
        "class DenseNetV2(DynamicDenseMlp):\n",
        "    def __init__(self, input_size, output_size, dropout_rate, hidden_sizes):\n",
        "        return super().__init__(input_size, output_size, dropout_rate, hidden_sizes=np.array([1,2,4,4,2,1]) * 512)\n",
        "    \n",
        "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate) -> list[Module]:\n",
        "        return [\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.Mish(),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        ]\n",
        "    \n",
        "    def describe() -> str:\n",
        "        return \"dense_mlp_v2_5layer\"\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mlwaKlDt_2c"
      },
      "source": [
        "#  Parameters Configuration and Create Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_class = NetV3\n",
        "config = {\n",
        "    'continue': False,\n",
        "    'last_epoch': 0,\n",
        "    'epochs'        : 100,\n",
        "    'batch_size'    : 32768,\n",
        "    'context'       : 40,\n",
        "    'init_lr'       : 1e-3,\n",
        "    'architecture'  : 'deep_mlp',\n",
        "    'dropout'       : 0.2,\n",
        "    'weight_decay'  : 1e-5,\n",
        "    'scheduler_params'     : {'patience': 3, 'factor': 0.2, 'min-lr': 1e-8},\n",
        "    # 'scheduler_params'     : {'tmax': 50},\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "    'wandb_name': f\"bs_32k_{model_class.describe()}_c40\"\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_ROOT = f\"/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/{config['wandb_name']}\"\n",
        "if not os.path.exists(MODEL_ROOT):\n",
        "    os.makedirs(MODEL_ROOT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "7xi7V8x8W9z4"
      },
      "outputs": [],
      "source": [
        "#TODO: Create a dataset object using the AudioDataset class for the training data\n",
        "train_data.set_context_length(config['context'])\n",
        "\n",
        "# TODO: Create a dataset object using the AudioDataset class for the validation data\n",
        "val_data.set_context_length(config['context'])\n",
        "\n",
        "# TODO: Create a dataset object using the AudioTestDataset class for the test data\n",
        "test_data.set_context_length(config['context'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "4mzoYfTKu14s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size     :  32768\n",
            "Context        :  40\n",
            "Input size     :  2187\n",
            "Output symbols :  42\n",
            "Train dataset samples = 36091157, batches = 1102\n",
            "Validation dataset samples = 1928204, batches = 59\n",
            "Test dataset samples = 1934138, batches = 60\n"
          ]
        }
      ],
      "source": [
        "# Define dataloaders for train, val and test datasets\n",
        "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
        "# We shuffle train dataloader but not val & test dataloader. Why?\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 8,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Batch size     : \", config['batch_size'])\n",
        "print(\"Context        : \", config['context'])\n",
        "print(\"Input size     : \", (2*config['context']+1)*27)\n",
        "print(\"Output symbols : \", len(PHONEMES))\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "# for i, data in enumerate(train_loader):\n",
        "#     frames, phoneme = data\n",
        "#     print(frames.shape, phoneme.shape)\n",
        "#     break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HejoSXe3vMVU"
      },
      "source": [
        "# Define Model, Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhGBH7-xxth"
      },
      "source": [
        "Here we define the model, loss function, optimizer and optionally a learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "_qtrEM1ZvLje"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "NetV3                                    [32768, 42]               --\n",
              "├─Sequential: 1-1                        [32768, 42]               --\n",
              "│    └─Linear: 2-1                       [32768, 512]              1,120,256\n",
              "│    └─Mish: 2-2                         [32768, 512]              --\n",
              "│    └─BatchNorm1d: 2-3                  [32768, 512]              1,024\n",
              "│    └─Dropout: 2-4                      [32768, 512]              --\n",
              "│    └─Linear: 2-5                       [32768, 1024]             525,312\n",
              "│    └─Mish: 2-6                         [32768, 1024]             --\n",
              "│    └─BatchNorm1d: 2-7                  [32768, 1024]             2,048\n",
              "│    └─Dropout: 2-8                      [32768, 1024]             --\n",
              "│    └─Linear: 2-9                       [32768, 2048]             2,099,200\n",
              "│    └─Mish: 2-10                        [32768, 2048]             --\n",
              "│    └─BatchNorm1d: 2-11                 [32768, 2048]             4,096\n",
              "│    └─Dropout: 2-12                     [32768, 2048]             --\n",
              "│    └─Linear: 2-13                      [32768, 4096]             8,392,704\n",
              "│    └─Mish: 2-14                        [32768, 4096]             --\n",
              "│    └─BatchNorm1d: 2-15                 [32768, 4096]             8,192\n",
              "│    └─Dropout: 2-16                     [32768, 4096]             --\n",
              "│    └─Linear: 2-17                      [32768, 2048]             8,390,656\n",
              "│    └─Mish: 2-18                        [32768, 2048]             --\n",
              "│    └─BatchNorm1d: 2-19                 [32768, 2048]             4,096\n",
              "│    └─Dropout: 2-20                     [32768, 2048]             --\n",
              "│    └─Linear: 2-21                      [32768, 1024]             2,098,176\n",
              "│    └─Mish: 2-22                        [32768, 1024]             --\n",
              "│    └─BatchNorm1d: 2-23                 [32768, 1024]             2,048\n",
              "│    └─Dropout: 2-24                     [32768, 1024]             --\n",
              "│    └─Linear: 2-25                      [32768, 512]              524,800\n",
              "│    └─Mish: 2-26                        [32768, 512]              --\n",
              "│    └─BatchNorm1d: 2-27                 [32768, 512]              1,024\n",
              "│    └─Dropout: 2-28                     [32768, 512]              --\n",
              "│    └─Linear: 2-29                      [32768, 42]               21,546\n",
              "==========================================================================================\n",
              "Total params: 23,195,178\n",
              "Trainable params: 23,195,178\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 760.06\n",
              "==========================================================================================\n",
              "Input size (MB): 286.65\n",
              "Forward/backward pass size (MB): 5916.59\n",
              "Params size (MB): 92.78\n",
              "Estimated Total Size (MB): 6296.03\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "INPUT_SIZE  = (2*config['context'] + 1) * 27 # Why is this the case?\n",
        "# model       = LowCutoffNet(INPUT_SIZE, len(train_data.phonemes)).to(device)\n",
        "if issubclass(model_class, DynamicMlpNet):\n",
        "    model = model_class(input_size=INPUT_SIZE, output_size=len(train_data.phonemes), dropout_rate=config['dropout']).to(device)\n",
        "elif issubclass(model_class, DynamicDenseMlp):\n",
        "    model = model_class(input_size=INPUT_SIZE, output_size=len(train_data.phonemes), dropout_rate=config['dropout'], hidden_sizes=[64, 64, 64, 64]).to(device)\n",
        "# Some upstream dependency bug in torchsummaryX, using torchinfo instead: \n",
        "import torchinfo\n",
        "torchinfo.summary(model, input_size=(config['batch_size'], INPUT_SIZE))\n",
        "# summary(model, frames.to(device))\n",
        "# Check number of parameters of your network\n",
        "# Remember, you are limited to 24 million parameters for HW1 (including ensembles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UROGEVJevKD-"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n",
        "# We use CE because the task is multi-class classification\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr= config['init_lr'], weight_decay=config['weight_decay'], ) #Defining Optimizer\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, \n",
        "    patience=config['scheduler_params']['patience'], \n",
        "    min_lr=config['scheduler_params']['min-lr'], \n",
        "    factor=config['scheduler_params']['factor'],\n",
        "    verbose=True\n",
        ")\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config['scheduler_params']['tmax'], eta_min=1e-7, last_epoch=-1, verbose=True)\n",
        "# Recommended : Define Scheduler for Learning Rate,\n",
        "# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n",
        "# You can refer to Pytorch documentation for more information on how to use them.\n",
        "\n",
        "# Is your training time very high?\n",
        "# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n",
        "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JgeNhx4x2-P"
      },
      "source": [
        "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XblOHEVtKab2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8wjPz7DHqKcL"
      },
      "outputs": [],
      "source": [
        "from torch.cuda.amp import GradScaler\n",
        "\n",
        "def train(model, dataloader, optimizer, criterion, scheduler, logger, log_freq=100, use_amp=True):\n",
        "    scaler = GradScaler()\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "    \n",
        "    if use_amp:\n",
        "        for i, (frames, phonemes) in enumerate(dataloader):\n",
        "            ### Initialize Gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ### Move Data to Device (Ideally GPU)\n",
        "            frames      = frames.to(device)\n",
        "            phonemes    = phonemes.to(device)\n",
        "\n",
        "            ### Forward Propagation\n",
        "            with torch.autocast(device_type='cuda', dtype=frames.dtype):\n",
        "                logits  = model(frames)\n",
        "                ### Loss Calculation\n",
        "               \n",
        "                loss    = criterion(logits, phonemes)\n",
        "\n",
        "            ### Backward Propagation\n",
        "            scaler.scale(loss).backward()\n",
        "\n",
        "            ### Gradient Descent\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        \n",
        "\n",
        "            tloss   += loss.item()\n",
        "            # print(torch.argmax(logits, dim= 1), phonemes.shape, tloss)\n",
        "            tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "            \n",
        "            # raise Exception(\"Shape mismatch\")\n",
        "            \n",
        "\n",
        "            batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                                acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "            batch_bar.update()\n",
        "\n",
        "            ### Release memory\n",
        "            del frames, phonemes, logits\n",
        "            if (i+1)%log_freq == 0:\n",
        "                logger(i, tloss / (i+1), tacc*100 / (i+1))\n",
        "            # torch.cuda.empty_cache()\n",
        "    else:\n",
        "        for i, (frames, phonemes) in enumerate(dataloader):\n",
        "            ### Initialize Gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            ### Move Data to Device (Ideally GPU)\n",
        "            frames      = frames.to(device)\n",
        "            phonemes    = phonemes.to(device)\n",
        "\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "                \n",
        "\n",
        "            ### Backward Propagation\n",
        "            loss.backward()\n",
        "\n",
        "            ### Gradient Descent\n",
        "            optimizer.step()\n",
        "            \n",
        "\n",
        "            tloss   += loss.item()\n",
        "            tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "            batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                                acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "            batch_bar.update()\n",
        "\n",
        "            ### Release memory\n",
        "            del frames, phonemes, logits\n",
        "            if (i+1)%log_freq == 0:\n",
        "                logger(i, tloss / (i+1), tacc*100 / (i+1))\n",
        "            # torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "        \n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    return vloss, vacc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMd_XxPku5qp"
      },
      "source": [
        "# Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIbhR1wwbgI"
      },
      "source": [
        "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n",
        "\n",
        "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "SCDYx5VEu6qI"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzyatcmu\u001b[0m (\u001b[33mschool_stuff\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/zzy/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"a07bacf1f6490c2d1a0d4e22dd08701319310f93\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "xvUnYd3Bw2up"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "wandb version 0.16.3 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/bs_32k_7layer_mlp_v3_c40/wandb/run-20240207_174507-4ek4p5lj</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/school_stuff/hw1p2/runs/4ek4p5lj' target=\"_blank\">bs_32k_7layer_mlp_v3_c40</a></strong> to <a href='https://wandb.ai/school_stuff/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/school_stuff/hw1p2' target=\"_blank\">https://wandb.ai/school_stuff/hw1p2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/school_stuff/hw1p2/runs/4ek4p5lj' target=\"_blank\">https://wandb.ai/school_stuff/hw1p2/runs/4ek4p5lj</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# # Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = config['wandb_name'], ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw1p2\", ### Project should be created in your wandb account\n",
        "    config  = config, ### Wandb Config for your run\n",
        "    dir = MODEL_ROOT ### Wandb local directory\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "wft15E_IxYFi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/bs_32k_7layer_mlp_v3_c40/wandb/run-20240207_174507-4ek4p5lj/files/model_arch.txt']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ### Save your model architecture as a string with str(model)\n",
        "model_arch  = str(model)\n",
        "\n",
        "# ### Save it in a txt file\n",
        "arch_file   = open(f\"{MODEL_ROOT}/model_arch.txt\", \"w\")\n",
        "file_write  = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "# ### log it in your wandb run with wandb.save()\n",
        "wandb.save(arch_file.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nclx_04fu7Dd"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLMWfEpyGOB"
      },
      "source": [
        "Now, it is time to finally run your ablations! Have fun!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def wandb_logger(iter, loss, acc):\n",
        "    wandb.log({\"train_loss\": loss, \"train_acc\": acc})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 69.6696%\tTrain Loss 0.9561\t Learning Rate 0.0010000\n",
            "\tVal Acc 77.7080%\tVal Loss 0.6700\n",
            "\n",
            "Epoch 2/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 78.0899%\tTrain Loss 0.6619\t Learning Rate 0.0010000\n",
            "\tVal Acc 80.3804%\tVal Loss 0.5851\n",
            "\n",
            "Epoch 3/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 79.8996%\tTrain Loss 0.6008\t Learning Rate 0.0010000\n",
            "\tVal Acc 81.3680%\tVal Loss 0.5531\n",
            "\n",
            "Epoch 4/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 80.8606%\tTrain Loss 0.5688\t Learning Rate 0.0010000\n",
            "\tVal Acc 81.9136%\tVal Loss 0.5359\n",
            "\n",
            "Epoch 5/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 81.4917%\tTrain Loss 0.5479\t Learning Rate 0.0010000\n",
            "\tVal Acc 82.3512%\tVal Loss 0.5228\n",
            "\n",
            "Epoch 6/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 81.9430%\tTrain Loss 0.5329\t Learning Rate 0.0010000\n",
            "\tVal Acc 82.5618%\tVal Loss 0.5168\n",
            "\n",
            "Epoch 7/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 82.2806%\tTrain Loss 0.5216\t Learning Rate 0.0010000\n",
            "\tVal Acc 82.8178%\tVal Loss 0.5084\n",
            "\n",
            "Epoch 8/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 82.5675%\tTrain Loss 0.5123\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.1056%\tVal Loss 0.5003\n",
            "\n",
            "Epoch 9/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 82.7980%\tTrain Loss 0.5043\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.0779%\tVal Loss 0.5009\n",
            "\n",
            "Epoch 10/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.0087%\tTrain Loss 0.4973\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.4159%\tVal Loss 0.4918\n",
            "\n",
            "Epoch 11/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.1811%\tTrain Loss 0.4917\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.4927%\tVal Loss 0.4885\n",
            "\n",
            "Epoch 12/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.3265%\tTrain Loss 0.4868\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.4793%\tVal Loss 0.4882\n",
            "\n",
            "Epoch 13/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.4513%\tTrain Loss 0.4826\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.7553%\tVal Loss 0.4806\n",
            "\n",
            "Epoch 14/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.5699%\tTrain Loss 0.4788\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.6604%\tVal Loss 0.4843\n",
            "\n",
            "Epoch 15/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.6706%\tTrain Loss 0.4754\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.8773%\tVal Loss 0.4772\n",
            "\n",
            "Epoch 16/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.7595%\tTrain Loss 0.4724\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.9644%\tVal Loss 0.4747\n",
            "\n",
            "Epoch 17/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.8423%\tTrain Loss 0.4698\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.8591%\tVal Loss 0.4797\n",
            "\n",
            "Epoch 18/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.9163%\tTrain Loss 0.4673\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.9399%\tVal Loss 0.4769\n",
            "\n",
            "Epoch 19/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 83.9848%\tTrain Loss 0.4650\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.8290%\tVal Loss 0.4810\n",
            "\n",
            "Epoch 20/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00020: reducing learning rate of group 0 to 2.0000e-04.\n",
            "\tTrain Acc 84.0579%\tTrain Loss 0.4628\t Learning Rate 0.0010000\n",
            "\tVal Acc 83.9426%\tVal Loss 0.4762\n",
            "\n",
            "Epoch 21/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 85.8544%\tTrain Loss 0.4031\t Learning Rate 0.0002000\n",
            "\tVal Acc 85.4330%\tVal Loss 0.4334\n",
            "\n",
            "Epoch 22/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 86.4898%\tTrain Loss 0.3814\t Learning Rate 0.0002000\n",
            "\tVal Acc 85.5476%\tVal Loss 0.4338\n",
            "\n",
            "Epoch 23/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 86.7503%\tTrain Loss 0.3722\t Learning Rate 0.0002000\n",
            "\tVal Acc 85.5104%\tVal Loss 0.4365\n",
            "\n",
            "Epoch 24/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 86.9320%\tTrain Loss 0.3662\t Learning Rate 0.0002000\n",
            "\tVal Acc 85.5130%\tVal Loss 0.4373\n",
            "\n",
            "Epoch 25/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00025: reducing learning rate of group 0 to 4.0000e-05.\n",
            "\tTrain Acc 87.0529%\tTrain Loss 0.3618\t Learning Rate 0.0002000\n",
            "\tVal Acc 85.5574%\tVal Loss 0.4372\n",
            "\n",
            "Epoch 26/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 87.5705%\tTrain Loss 0.3451\t Learning Rate 0.0000400\n",
            "\tVal Acc 85.8006%\tVal Loss 0.4333\n",
            "\n",
            "Epoch 27/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 87.7658%\tTrain Loss 0.3388\t Learning Rate 0.0000400\n",
            "\tVal Acc 85.8137%\tVal Loss 0.4344\n",
            "\n",
            "Epoch 28/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 87.8581%\tTrain Loss 0.3357\t Learning Rate 0.0000400\n",
            "\tVal Acc 85.8573%\tVal Loss 0.4338\n",
            "\n",
            "Epoch 29/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 87.9350%\tTrain Loss 0.3333\t Learning Rate 0.0000400\n",
            "\tVal Acc 85.8454%\tVal Loss 0.4346\n",
            "\n",
            "Epoch 30/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00030: reducing learning rate of group 0 to 8.0000e-06.\n",
            "\tTrain Acc 87.9918%\tTrain Loss 0.3313\t Learning Rate 0.0000400\n",
            "\tVal Acc 85.8486%\tVal Loss 0.4348\n",
            "\n",
            "Epoch 31/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.0983%\tTrain Loss 0.3278\t Learning Rate 0.0000080\n",
            "\tVal Acc 85.8830%\tVal Loss 0.4350\n",
            "\n",
            "Epoch 32/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.1309%\tTrain Loss 0.3267\t Learning Rate 0.0000080\n",
            "\tVal Acc 85.8894%\tVal Loss 0.4351\n",
            "\n",
            "Epoch 33/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.1522%\tTrain Loss 0.3260\t Learning Rate 0.0000080\n",
            "\tVal Acc 85.8897%\tVal Loss 0.4351\n",
            "\n",
            "Epoch 34/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00034: reducing learning rate of group 0 to 1.6000e-06.\n",
            "\tTrain Acc 88.1706%\tTrain Loss 0.3254\t Learning Rate 0.0000080\n",
            "\tVal Acc 85.8869%\tVal Loss 0.4352\n",
            "\n",
            "Epoch 35/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.1937%\tTrain Loss 0.3246\t Learning Rate 0.0000016\n",
            "\tVal Acc 85.8953%\tVal Loss 0.4352\n",
            "\n",
            "Epoch 36/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2015%\tTrain Loss 0.3245\t Learning Rate 0.0000016\n",
            "\tVal Acc 85.8929%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 37/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2082%\tTrain Loss 0.3242\t Learning Rate 0.0000016\n",
            "\tVal Acc 85.8896%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 38/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00038: reducing learning rate of group 0 to 3.2000e-07.\n",
            "\tTrain Acc 88.2114%\tTrain Loss 0.3242\t Learning Rate 0.0000016\n",
            "\tVal Acc 85.8862%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 39/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2146%\tTrain Loss 0.3240\t Learning Rate 0.0000003\n",
            "\tVal Acc 85.8954%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 40/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2183%\tTrain Loss 0.3241\t Learning Rate 0.0000003\n",
            "\tVal Acc 85.8911%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 41/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2168%\tTrain Loss 0.3240\t Learning Rate 0.0000003\n",
            "\tVal Acc 85.8931%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 42/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00042: reducing learning rate of group 0 to 6.4000e-08.\n",
            "\tTrain Acc 88.2185%\tTrain Loss 0.3239\t Learning Rate 0.0000003\n",
            "\tVal Acc 85.8953%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 43/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2194%\tTrain Loss 0.3239\t Learning Rate 0.0000001\n",
            "\tVal Acc 85.8900%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 44/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2129%\tTrain Loss 0.3241\t Learning Rate 0.0000001\n",
            "\tVal Acc 85.8936%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 45/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2292%\tTrain Loss 0.3238\t Learning Rate 0.0000001\n",
            "\tVal Acc 85.8920%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 46/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 00046: reducing learning rate of group 0 to 1.2800e-08.\n",
            "\tTrain Acc 88.2155%\tTrain Loss 0.3240\t Learning Rate 0.0000001\n",
            "\tVal Acc 85.8879%\tVal Loss 0.4357\n",
            "\n",
            "Epoch 47/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2210%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8947%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 48/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2248%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8933%\tVal Loss 0.4352\n",
            "\n",
            "Epoch 49/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2231%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8988%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 50/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2245%\tTrain Loss 0.3237\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8949%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 51/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2231%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8943%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 52/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2181%\tTrain Loss 0.3240\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8974%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 53/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2194%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8905%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 54/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2209%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8918%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 55/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2119%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8886%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 56/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2208%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8922%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 57/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2200%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8929%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 58/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2128%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8936%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 59/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2130%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8893%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 60/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2162%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8926%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 61/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2210%\tTrain Loss 0.3237\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8937%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 62/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2187%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8927%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 63/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2146%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8878%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 64/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2240%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8886%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 65/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2207%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8924%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 66/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2245%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8920%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 67/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2223%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8919%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 68/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2188%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8928%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 69/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2266%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8898%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 70/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2140%\tTrain Loss 0.3240\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8945%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 71/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2221%\tTrain Loss 0.3237\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8902%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 72/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2183%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8919%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 73/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2214%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8942%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 74/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2222%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8887%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 75/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2142%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8921%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 76/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2232%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8920%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 77/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2183%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8906%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 78/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2221%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8891%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 79/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2200%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8943%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 80/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2222%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8912%\tVal Loss 0.4352\n",
            "\n",
            "Epoch 81/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2213%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8930%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 82/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2232%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8907%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 83/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2228%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8883%\tVal Loss 0.4357\n",
            "\n",
            "Epoch 84/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2181%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8898%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 85/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2239%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8918%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 86/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2234%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8925%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 87/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2230%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8943%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 88/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2206%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8930%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 89/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2207%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8941%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 90/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2263%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8899%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 91/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2226%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8926%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 92/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2268%\tTrain Loss 0.3237\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8963%\tVal Loss 0.4353\n",
            "\n",
            "Epoch 93/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2192%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8940%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 94/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2163%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8946%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 95/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2190%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8942%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 96/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2205%\tTrain Loss 0.3239\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8927%\tVal Loss 0.4356\n",
            "\n",
            "Epoch 97/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2196%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8930%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 98/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2253%\tTrain Loss 0.3238\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8913%\tVal Loss 0.4354\n",
            "\n",
            "Epoch 99/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2268%\tTrain Loss 0.3237\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8964%\tVal Loss 0.4355\n",
            "\n",
            "Epoch 100/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tTrain Acc 88.2174%\tTrain Loss 0.3240\t Learning Rate 0.0000000\n",
            "\tVal Acc 85.8944%\tVal Loss 0.4354\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>████████▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▁▃▄▄▅▅▅▅▇▇██████████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▅▅▄▄▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▆▆▆████████████████████████████████</td></tr><tr><td>valid_loss</td><td>█▅▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.0</td></tr><tr><td>train_acc</td><td>88.21744</td></tr><tr><td>train_loss</td><td>0.32397</td></tr><tr><td>val_acc</td><td>85.8944</td></tr><tr><td>valid_loss</td><td>0.43535</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bs_32k_7layer_mlp_v3_c40</strong> at: <a href='https://wandb.ai/school_stuff/hw1p2/runs/4ek4p5lj' target=\"_blank\">https://wandb.ai/school_stuff/hw1p2/runs/4ek4p5lj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/bs_32k_7layer_mlp_v3_c40/wandb/run-20240207_174507-4ek4p5lj/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "wandb.watch(model, log=\"all\")\n",
        "\n",
        "\n",
        "#Continue training\n",
        "if config['continue']:\n",
        "    last_epoch = config['last_epoch']\n",
        "    model.load_state_dict(torch.load(f\"{MODEL_ROOT}/model_{last_epoch}.cpt\"))\n",
        "    initial_epoch = last_epoch + 1\n",
        "    scheduler.last_epoch = initial_epoch\n",
        "else:\n",
        "    initial_epoch = 0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "    if epoch < initial_epoch:\n",
        "        continue\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion, scheduler, logger=wandb_logger, log_freq=100, use_amp=False)\n",
        "    \n",
        "    val_loss, val_acc       = eval(model, val_loader)\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
        "\n",
        "    ### Log metrics at each epoch in your run\n",
        "    # Optionally, you can log at each batch inside train/eval functions\n",
        "    # (explore wandb documentation/wandb recitation)\n",
        "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "    \n",
        "\n",
        "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
        "    torch.save(model.state_dict(), f\"{MODEL_ROOT}/model_{epoch}.cpt\")\n",
        "\n",
        "### Finish your wandb run\n",
        "run.finish()\n",
        "# Baseline model 1st epoch:\n",
        "#   Train Acc 51.7162%\tTrain Loss 1.6885\t Learning Rate 0.0010000\n",
        "#\tVal Acc 63.6595%\tVal Loss 1.1859\n",
        "#7-layer v3 model 1st epoch:\n",
        "#   Train Acc 69.9118%\tTrain Loss 0.9459\t Learning Rate 0.0010000\n",
        "#\tVal Acc 77.559%\tVal Loss 0.6768\n",
        "#8layer v4 model 1st epoch:\n",
        "#   Train Acc 69.1642%\tTrain Loss 0.9956\t Learning Rate 0.0010000\n",
        "#\tVal Acc 76.8617%\tVal Loss 0.7113\n",
        "#dense v2 model 1st epoch:\n",
        "#   Train Acc 68.18843%\tTrain Loss 1.0120\t Learning Rate 0.0010000\n",
        "#\tVal Acc 76.7886%\tVal Loss 0.7109"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXwf5YUo_4A"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1hSFYLpJvH"
      },
      "source": [
        "Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    ### What you call for model to perform inference?\n",
        "    model.eval() # TODO train or eval?\n",
        "\n",
        "    ### List to store predicted phonemes of test data\n",
        "    test_predictions = []\n",
        "\n",
        "    ### Which mode do you need to avoid gradients?\n",
        "    with torch.no_grad(): \n",
        "\n",
        "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
        "\n",
        "            mfccs   = mfccs.to(device)\n",
        "\n",
        "            logits  = model(mfccs)\n",
        "\n",
        "            ### Get most likely predicted phoneme with argmax\n",
        "            max_idxs = torch.argmax(logits, dim=1)\n",
        "            \n",
        "            predicted_phonemes = [test_loader.dataset.phoneme_reverse_lookup(max_idx) for max_idx in max_idxs]\n",
        "\n",
        "            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n",
        "            test_predictions.extend(predicted_phonemes)\n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "wG9v6Xmxu7wp"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 60/60 [01:09<00:00,  1.16s/it]\n"
          ]
        }
      ],
      "source": [
        "predictions = test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "### Create CSV file with predictions\n",
        "with open(f\"{MODEL_ROOT}/submission_latest.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(predictions)):\n",
        "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "LjcammuCxMKN"
      },
      "outputs": [],
      "source": [
        "### Submit to kaggle competition using kaggle API (Uncomment below to use)\n",
        "# !kaggle competitions submit -c 11785-hw1p2-s24 -f ./submission.csv -m \"Test Submission\"\n",
        "\n",
        "### However, its always safer to download the csv file and then upload to kaggle"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
