{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW1: Frame-Level Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Preparation\n",
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: NVIDIA GeForce RTX 4090\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from torchsummaryX import summary\n",
    "import torchinfo\n",
    "import sklearn\n",
    "import gc\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import datetime\n",
    "import wandb\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print('Device:', 'cpu' if device == 'cpu' else torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_mkdir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "        print(f\"Created directory: {dir_path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists: {dir_path}\")\n",
    "\n",
    "PHONEMES = [\n",
    "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
    "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
    "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
    "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
    "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
    "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']\n",
    "DATA_ROOT = \"/mnt/e/Workspace/IDL/Data/hw1/11-785-s24-hw1p2/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import time\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, root=DATA_ROOT, phonemes = PHONEMES, context=0, partition= \"train-clean-100\", use_cmn=False): #TODO: make no-pad works (BF2042 meme huh?) \n",
    "        self.max_context_length = 1145 #Magic number\n",
    "        self.set_context_length(context)\n",
    "        self.phonemes   = phonemes\n",
    "        \n",
    "        self.num_phonemes = len(self.phonemes)\n",
    "        \n",
    "        self.mfccs, self.transcripts = self._init_data(f\"{root}/{partition}\", use_cmn=use_cmn)\n",
    "        self.length = len(self.mfccs)\n",
    "        \n",
    "        self.mfccs = np.concatenate([np.zeros((self.max_context_length, 27)), self.mfccs, np.zeros((self.max_context_length, 27))], axis=0)\n",
    "        self.transcripts = np.concatenate([\n",
    "                [self.phonemes.index('[SIL]') for _ in range(self.max_context_length)], \n",
    "                self.transcripts, \n",
    "                [self.phonemes.index('[SIL]') for _ in range(self.max_context_length)]\n",
    "            ],axis=0)\n",
    "        assert len(self.mfccs) == len(self.transcripts)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start = time.perf_counter_ns()\n",
    "        # given current context length, compute offset:\n",
    "        lower = idx + self.max_context_length - self.context\n",
    "        upper = idx + self.max_context_length + self.context + 1\n",
    "            \n",
    "\n",
    "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
    "        frames = self.mfccs[lower:upper]\n",
    "        \n",
    "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
    "        frames = frames.flatten() # Reshape to 1d array\n",
    "\n",
    "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
    "        phonemes    = torch.tensor(self.transcripts[idx + self.max_context_length]) # Get the phoneme at the index\n",
    "\n",
    "        return frames, phonemes\n",
    "        \n",
    "        \n",
    "    def _init_data(self, root: str, use_cmn = False):\n",
    "        self.mfcc_dir       = f\"{root}/mfcc\"\n",
    "        self.transcript_dir = f\"{root}/transcript\"\n",
    "        mfcc_names          = os.listdir(self.mfcc_dir)\n",
    "        transcript_names    = os.listdir(self.transcript_dir)\n",
    "        \n",
    "        assert len(mfcc_names) == len(transcript_names)\n",
    "\n",
    "        self.mfccs, self.transcripts = [], []\n",
    "        for i in tqdm(range(len(mfcc_names))):\n",
    "        #   Load a single mfcc\n",
    "            mfcc        = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
    "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
    "            if use_cmn:\n",
    "                mfcc = mfcc - np.mean(mfcc, axis=0)\n",
    "        #   Load the corresponding transcript\n",
    "            transcript  = np.load(f\"{self.transcript_dir}/{transcript_names[i]}\") \n",
    "            # Remove [SOS] and [EOS] from the transcript\n",
    "            assert transcript[0] == '[SOS]' and transcript[-1] == '[EOS]'\n",
    "            transcript = transcript[1:-1]\n",
    "            #lookup phoneme index\n",
    "            transcript = np.vectorize(self.phonemes.index)(transcript)\n",
    "            assert len(mfcc) == len(transcript)\n",
    "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
    "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
    "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
    "            self.mfccs.append(mfcc)\n",
    "            self.transcripts.append(transcript)\n",
    "            \n",
    "        return np.concatenate(self.mfccs, axis=0), np.concatenate(self.transcripts, axis=0)\n",
    "    \n",
    "    def set_context_length(self, context):\n",
    "        self.context = context\n",
    "        \n",
    "    def phoneme_reverse_lookup(self, idx: torch.tensor) -> str:\n",
    "        return self.phonemes[idx]\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTestDataset(AudioDataset):\n",
    "    \n",
    "    def _init_data(self, root: str, use_cmn):\n",
    "        \n",
    "        self.mfcc_dir = f\"{root}/mfcc\"\n",
    "\n",
    "        mfcc_names = os.listdir(self.mfcc_dir)\n",
    "\n",
    "        self.mfccs, self.transcripts = [], []\n",
    "        \n",
    "        for i in tqdm(range(len(mfcc_names))):\n",
    "        #   Load a single mfcc\n",
    "            mfcc        = np.load(f\"{self.mfcc_dir}/{mfcc_names[i]}\")\n",
    "            transcript = np.array([0 for _ in range(len(mfcc))])\n",
    "            \n",
    "            assert len(mfcc) == len(transcript)\n",
    "            \n",
    "            self.mfccs.append(mfcc)\n",
    "            self.transcripts.append(transcript)\n",
    "            \n",
    "        return np.concatenate(self.mfccs, axis=0), np.concatenate(self.transcripts, axis=0)\n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return super().__getitem__(ind)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/28539 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28539/28539 [03:09<00:00, 150.51it/s]\n",
      "100%|██████████| 2703/2703 [00:21<00:00, 128.64it/s]\n",
      "100%|██████████| 2620/2620 [00:09<00:00, 280.22it/s]\n"
     ]
    }
   ],
   "source": [
    "test_context = 5\n",
    "\n",
    "train_data = AudioDataset(partition=\"train-clean-100\", context=test_context, use_cmn=True)\n",
    "\n",
    "val_data = AudioDataset(partition=\"dev-clean\", context=test_context, use_cmn=True)\n",
    "\n",
    "test_data = AudioTestDataset(partition=\"test-clean\", context=test_context, use_cmn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample frame shape: torch.Size([297]) Sample phoneme shape: torch.Size([]) phoneme type: torch.int64\n",
      "Sample frame shape: torch.Size([297])\n"
     ]
    }
   ],
   "source": [
    "# Tests:\n",
    "\n",
    "f, p = val_data[0]\n",
    "print('Sample frame shape:', f.shape, 'Sample phoneme shape:', p.shape, 'phoneme type:', p.dtype)\n",
    "f = test_data[0]\n",
    "print('Sample frame shape:', f.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NetV2(torch.nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, dropout_rate):\n",
    "        super(NetV2, self).__init__()\n",
    "        self.layers = []\n",
    "        for i, hs in enumerate(hidden_sizes):\n",
    "            self.layers.append(self._mlp_layer_provider(input_size, hs, dropout_rate))\n",
    "            input_size = hs\n",
    "        self.layers.append(nn.Linear(input_size, output_size)) # output\n",
    "        self.model = nn.Sequential(*self.layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "    \n",
    "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetV21(NetV2):\n",
    "    def _mlp_layer_provider(self, input_size, hidden_size, dropout_rate):\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.Mish(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: /mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/v2_1_5l_4k_bs_256_size\n",
      "Batch size     :  32768\n",
      "Context        :  20\n",
      "Input size     :  1107\n",
      "Output symbols :  42\n",
      "Train dataset samples = 36091157, batches = 1102\n",
      "Validation dataset samples = 1928204, batches = 59\n",
      "Test dataset samples = 1934138, batches = 60\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    'sizes': np.array([1,2,4,2,1]) * 512, # 5 layers\n",
    "    'epochs'        : 30,\n",
    "    'batch_size'    : 32768,\n",
    "    'context'       : 20,\n",
    "    'init_lr'       : 2e-3,\n",
    "    'architecture'  : 'v2_1_5layers',\n",
    "    'dropout'       : 0.2,\n",
    "    'weight_decay'  : 1e-5,\n",
    "    'scheduler_params'     : {'patience': 7, 'factor': 0.2, 'min-lr': 1e-7},\n",
    "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
    "    'wandb_name': 'v2_1_5layers_4k_bs'\n",
    "}\n",
    "\n",
    "train_data.set_context_length(config['context'])\n",
    "val_data.set_context_length(config['context'])\n",
    "test_data.set_context_length(config['context'])\n",
    "\n",
    "MODEL_ROOT = \"/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/v2_1_5l_4k_bs_256_size\"\n",
    "recursive_mkdir(MODEL_ROOT)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = train_data,\n",
    "    num_workers = 4,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = val_data,\n",
    "    num_workers = 4,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset     = test_data,\n",
    "    num_workers = 4,\n",
    "    batch_size  = config['batch_size'],\n",
    "    pin_memory  = True,\n",
    "    shuffle     = False\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Batch size     : \", config['batch_size'])\n",
    "print(\"Context        : \", config['context'])\n",
    "print(\"Input size     : \", (2*config['context']+1)*27)\n",
    "print(\"Output symbols : \", len(PHONEMES))\n",
    "\n",
    "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
    "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
    "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32768, 1107]) torch.Size([32768])\n"
     ]
    }
   ],
   "source": [
    "# Testing code to check if your data loaders are working\n",
    "for i, data in enumerate(train_loader):\n",
    "    frames, phoneme = data\n",
    "    print(frames.shape, phoneme.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "NetV21                                   [32768, 42]               --\n",
       "├─Sequential: 1-1                        [32768, 42]               --\n",
       "│    └─Sequential: 2-1                   [32768, 512]              --\n",
       "│    │    └─Linear: 3-1                  [32768, 512]              567,296\n",
       "│    │    └─Mish: 3-2                    [32768, 512]              --\n",
       "│    │    └─Dropout: 3-3                 [32768, 512]              --\n",
       "│    └─Sequential: 2-2                   [32768, 1024]             --\n",
       "│    │    └─Linear: 3-4                  [32768, 1024]             525,312\n",
       "│    │    └─Mish: 3-5                    [32768, 1024]             --\n",
       "│    │    └─Dropout: 3-6                 [32768, 1024]             --\n",
       "│    └─Sequential: 2-3                   [32768, 2048]             --\n",
       "│    │    └─Linear: 3-7                  [32768, 2048]             2,099,200\n",
       "│    │    └─Mish: 3-8                    [32768, 2048]             --\n",
       "│    │    └─Dropout: 3-9                 [32768, 2048]             --\n",
       "│    └─Sequential: 2-4                   [32768, 1024]             --\n",
       "│    │    └─Linear: 3-10                 [32768, 1024]             2,098,176\n",
       "│    │    └─Mish: 3-11                   [32768, 1024]             --\n",
       "│    │    └─Dropout: 3-12                [32768, 1024]             --\n",
       "│    └─Sequential: 2-5                   [32768, 512]              --\n",
       "│    │    └─Linear: 3-13                 [32768, 512]              524,800\n",
       "│    │    └─Mish: 3-14                   [32768, 512]              --\n",
       "│    │    └─Dropout: 3-15                [32768, 512]              --\n",
       "│    └─Linear: 2-6                       [32768, 42]               21,546\n",
       "==========================================================================================\n",
       "Total params: 5,836,330\n",
       "Trainable params: 5,836,330\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 191.24\n",
       "==========================================================================================\n",
       "Input size (MB): 145.10\n",
       "Forward/backward pass size (MB): 1353.19\n",
       "Params size (MB): 23.35\n",
       "Estimated Total Size (MB): 1521.63\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary:\n",
    "INPUT_SIZE = (2*config['context']+1)*27\n",
    "# model = NetV2(input_size=INPUT_SIZE, output_size=len(PHONEMES), hidden_sizes=config['sizes'], dropout_rate=config['dropout']).to(device)\n",
    "model = NetV21(input_size=INPUT_SIZE, output_size=len(PHONEMES), hidden_sizes=config['sizes'], dropout_rate=config['dropout']).to(device)\n",
    "torchinfo.summary(model, input_size=(config['batch_size'], INPUT_SIZE), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n",
    "# We use CE because the task is multi-class classification\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= config['init_lr'], weight_decay=config['weight_decay'], ) #Defining Optimizer\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    patience=config['scheduler_params']['patience'], \n",
    "    min_lr=config['scheduler_params']['min-lr'], \n",
    "    factor=config['scheduler_params']['factor'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "## funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "#A epoch:\n",
    "def train(model, dataloader, optimizer, criterion, scheduler, logger, log_freq=100, use_amp=False):\n",
    "    if use_amp: raise NotImplementedError(\"AMP not implemented yet\")\n",
    "    else: return train_no_amp(model, dataloader, optimizer, criterion, scheduler, logger, log_freq)\n",
    "    \n",
    "def train_no_amp(model, dataloader, optimizer, criterion, scheduler, logger, log_freq=100):\n",
    "    model.train()\n",
    "    total_loss, total_acc = 0.0, 0.0\n",
    "    \n",
    "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "    for i, data in enumerate(dataloader):\n",
    "        frames, phonemes = data\n",
    "        frames, phonemes = frames.to(device), phoneme.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(frames)\n",
    "        loss = criterion(logits, phonemes)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # scheduler.step(loss)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_acc += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
    "        \n",
    "        del frames, phonemes, logits\n",
    "        \n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
    "                                acc=\"{:.04f}%\".format(float(total_acc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "        \n",
    "        #Switch:\n",
    "        # raise EOFError('Kill switch')\n",
    "        \n",
    "        if (i+1) % log_freq == 0:\n",
    "            logger(i, total_loss / (i+1), total_acc / (i+1))\n",
    "    \n",
    "    batch_bar.close()\n",
    "    total_loss   /= len(train_loader)\n",
    "    total_acc /= len(train_loader)\n",
    "    return total_loss, total_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, dataloader):\n",
    "\n",
    "    model.eval() # set model in evaluation mode\n",
    "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
    "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
    "\n",
    "    for i, (frames, phonemes) in enumerate(dataloader):\n",
    "\n",
    "        ### Move data to device (ideally GPU)\n",
    "        frames      = frames.to(device)\n",
    "        phonemes    = phonemes.to(device)\n",
    "\n",
    "        # makes sure that there are no gradients computed as we are not training the model now\n",
    "        with torch.inference_mode():\n",
    "            ### Forward Propagation\n",
    "            logits  = model(frames)\n",
    "            ### Loss Calculation\n",
    "            loss    = criterion(logits, phonemes)\n",
    "\n",
    "        vloss   += loss.item()\n",
    "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
    "\n",
    "        # Do you think we need loss.backward() and optimizer.step() here?\n",
    "\n",
    "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
    "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
    "        batch_bar.update()\n",
    "\n",
    "        ### Release memory\n",
    "        del frames, phonemes, logits\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "\n",
    "    batch_bar.close()\n",
    "    vloss   /= len(val_loader)\n",
    "    vacc    /= len(val_loader)\n",
    "\n",
    "    return vloss, vacc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wandb run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzzyatcmu\u001b[0m (\u001b[33mschool_stuff\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/zzy/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/e/Workspace/IDL/Models/hw1/11-785-s24-hw1p2/v2_1_5l_4k_bs_256_size/wandb/run-20240206_034701-jjzqoisp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/school_stuff/hw1p2/runs/jjzqoisp' target=\"_blank\">v2_1_5layers_4k_bs</a></strong> to <a href='https://wandb.ai/school_stuff/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/school_stuff/hw1p2' target=\"_blank\">https://wandb.ai/school_stuff/hw1p2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/school_stuff/hw1p2/runs/jjzqoisp' target=\"_blank\">https://wandb.ai/school_stuff/hw1p2/runs/jjzqoisp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Saving files without folders. If you want to preserve sub directories pass base_path to wandb.save, i.e. wandb.save(\"/mnt/folder/file.h5\", base_path=\"/mnt\")\n"
     ]
    }
   ],
   "source": [
    "wandb.login(key=\"a07bacf1f6490c2d1a0d4e22dd08701319310f93\") \n",
    "run = wandb.init(\n",
    "    name    = config['wandb_name'], ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
    "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
    "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
    "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
    "    project = \"hw1p2\", ### Project should be created in your wandb account\n",
    "    config  = config, ### Wandb Config for your run\n",
    "    dir = MODEL_ROOT ### Wandb local directory\n",
    ")\n",
    "\n",
    "model_arch  = str(model)\n",
    "\n",
    "# ### Save it in a txt file\n",
    "arch_file   = open(f\"{MODEL_ROOT}/model_arch.txt\", \"w\")\n",
    "file_write  = arch_file.write(model_arch)\n",
    "arch_file.close()\n",
    "\n",
    "# ### log it in your wandb run with wandb.save()\n",
    "wandb.save(arch_file.name)\n",
    "\n",
    "def wandb_logger(epoch, loss, acc):\n",
    "    wandb.log({\"train_epoch\": epoch, \"train_loss\": loss, \"train_acc\": acc})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   8%|▊         | 86/1102 [00:11<02:03,  8.23it/s, acc=16.2386%, loss=3.4907]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      9\u001b[0m curr_lr                 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 10\u001b[0m train_loss, train_acc   \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_amp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(train_loss)\n\u001b[1;32m     12\u001b[0m val_loss, val_acc       \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(model, val_loader)\n",
      "Cell \u001b[0;32mIn[14], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, dataloader, optimizer, criterion, scheduler, logger, log_freq, use_amp)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(model, dataloader, optimizer, criterion, scheduler, logger, log_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, use_amp\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_amp: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP not implemented yet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_no_amp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m, in \u001b[0;36mtrain_no_amp\u001b[0;34m(model, dataloader, optimizer, criterion, scheduler, logger, log_freq)\u001b[0m\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# scheduler.step(loss)\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m total_acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m phonemes)\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;241m/\u001b[39mlogits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m frames, phonemes, logits\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   8%|▊         | 86/1102 [00:25<02:03,  8.23it/s, acc=16.2386%, loss=3.4907]"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "wandb.watch(model, log=\"all\")\n",
    "\n",
    "\n",
    "for epoch in range(config['epochs']):\n",
    "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
    "\n",
    "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
    "    train_loss, train_acc   = train(model, train_loader, optimizer, criterion, scheduler, logger=wandb_logger, log_freq=100, use_amp=False)\n",
    "    scheduler.step(train_loss)\n",
    "    val_loss, val_acc       = eval(model, val_loader)\n",
    "\n",
    "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
    "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
    "\n",
    "    ### Log metrics at each epoch in your run\n",
    "    # Optionally, you can log at each batch inside train/eval functions\n",
    "    # (explore wandb documentation/wandb recitation)\n",
    "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
    "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
    "    \n",
    "\n",
    "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n",
    "    torch.save(model.state_dict(), f\"{MODEL_ROOT}/model_{epoch}.cpt\")\n",
    "\n",
    "### Finish your wandb run\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader):\n",
    "    ### What you call for model to perform inference?\n",
    "    model.eval() # TODO train or eval?\n",
    "\n",
    "    ### List to store predicted phonemes of test data\n",
    "    test_predictions = []\n",
    "\n",
    "    ### Which mode do you need to avoid gradients?\n",
    "    with torch.no_grad(): \n",
    "\n",
    "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
    "\n",
    "            mfccs   = mfccs.to(device)\n",
    "\n",
    "            logits  = model(mfccs)\n",
    "\n",
    "            ### Get most likely predicted phoneme with argmax\n",
    "            max_idxs = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predicted_phonemes = [test_loader.dataset.phoneme_reverse_lookup(max_idx) for max_idx in max_idxs]\n",
    "\n",
    "            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n",
    "            test_predictions.extend(predicted_phonemes)\n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test(model, test_loader)\n",
    "with open(\"./submission_latest.csv\", \"w+\") as f:\n",
    "    f.write(\"id,label\\n\")\n",
    "    for i in range(len(predictions)):\n",
    "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "11785",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
