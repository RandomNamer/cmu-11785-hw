{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSLkT0qL3jgl"
      },
      "source": [
        "# **HW4P1: Language Modelling**\n",
        "\n",
        "Welcome to the final Part 1 HW of this course. This is the only part 1 in which you have PyTorch training (Yay!). You will be working on training language models and evaluating them on the task of prediction and generation.<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TbDdinS6buu"
      },
      "source": [
        "As you go, please read the code and keep an eye out for TODOs.\n",
        "\n",
        "Structure of this notebook:\n",
        "\n",
        "- **Imports and installs** - specify the correct data paths and mostly just run it.\n",
        "- **Datasets** - complete TODO and run it.\n",
        "- **Dataloader** - complete TODO and run it.\n",
        "- **Language model architecture** - implement and define your preferred model architecture based on the writeup.\n",
        "- **Dataloader, model, loss, optimizer, and scheduler definition** - define your dataloader, model, loss, optimizer, and scheduler.\n",
        "- **Trainer class** - unlike all the P2s, we are using a Trainer class for this HW, review the class and complete the train function.\n",
        "- **Wandb** - add a correct API key.\n",
        "- **Experiments** - just run your experiments and note the resulting NLL metric.\n",
        "- **Evaluation** - get access to OpenAI API to get the resulting perplexity metric.\n",
        "- **Submission** - create a handin for Autolab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "95e48c7693e34a389da49dcb6e448e0c",
        "deepnote_cell_type": "markdown",
        "id": "EB2bOV3bzYLR"
      },
      "source": [
        "# **Imports and installs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tG9-HVopYWzX"
      },
      "outputs": [],
      "source": [
        "# !tar -xvf /content/hw4p1_handout.tar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "r4_-qG9rSULt"
      },
      "outputs": [],
      "source": [
        "# !pip install torchsummaryX\n",
        "# !pip install wandb --quiet\n",
        "# !pip install matplotlib\n",
        "\n",
        "# !pip install -q cohere tiktoken openai\n",
        "# !pip install openai==0.28``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RnrUvEIC5i5j"
      },
      "outputs": [],
      "source": [
        "# TODO: Import drive if you are using Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cell_id": "03bf3bd639a048f098d5febc42e2baff",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 4,
        "execution_start": 1679856365820,
        "id": "QZNwme4320LW",
        "source_hash": "b7876178"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/mnt/w/Sync/Courses/24 Spring/11785 IDL/hw/hw4/HW4P1/hw4',\n",
              " '',\n",
              " '/opt/ros/humble/lib/python3.10/site-packages',\n",
              " '/opt/ros/humble/local/lib/python3.10/dist-packages',\n",
              " '/home/zzy/miniconda3/envs/11785/lib/python311.zip',\n",
              " '/home/zzy/miniconda3/envs/11785/lib/python3.11',\n",
              " '/home/zzy/miniconda3/envs/11785/lib/python3.11/lib-dynload',\n",
              " '/home/zzy/miniconda3/envs/11785/lib/python3.11/site-packages']"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# You can upload entire working directory of HW4P1 to google drive and access the files from there\n",
        "\n",
        "import sys\n",
        "sys.path\n",
        "# path = NotImplemented # TODO: Add path to handout. For example \"content/handout\"\n",
        "# sys.path.append(path)\n",
        "# %cd {path}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cell_id": "b48a9e95f26c4d2e89d95b1b311cedd5",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:09.992480Z",
          "iopub.status.busy": "2022-08-10T14:02:09.987693Z",
          "iopub.status.idle": "2022-08-10T14:02:12.872562Z",
          "shell.execute_reply": "2022-08-10T14:02:12.870819Z",
          "shell.execute_reply.started": "2022-08-10T14:02:09.991351Z"
        },
        "execution_millis": 2669,
        "execution_start": 1679856365830,
        "id": "oxiZ42B4SwQ-",
        "source_hash": "ec149d26",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "\n",
        "import os\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torchsummaryX\n",
        "import gc\n",
        "import wandb\n",
        "import yaml\n",
        "import openai\n",
        "\n",
        "# Importing necessary modules from hw4\n",
        "# Update the path depending on how you choose to load the handout\n",
        "from tests_hw4 import get_prediction_nll, make_generation_text\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# # print(\"Device: \", DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "a4ff875589ee46da8f749a7e5088a3ef",
        "deepnote_cell_type": "markdown",
        "id": "u-R794-0zc9V"
      },
      "source": [
        "# **Datasets**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HU4e_6l0Whda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocab length:  33280\n",
            "['!' '\"' '#' ... 'ï½ž' '<sos>' '<eos>']\n"
          ]
        }
      ],
      "source": [
        "# Loading the vocabulary. Try printing and see\n",
        "VOCAB       = np.load('../dataset/vocab.npy')\n",
        "\n",
        "# We have also included <sos> and <eos> in the vocabulary for you\n",
        "# However in real life, you include it explicitly if not provided\n",
        "SOS_TOKEN   = np.where(VOCAB == '<sos>')[0][0]\n",
        "EOS_TOKEN   = np.where(VOCAB == '<eos>')[0][0]\n",
        "NUM_WORDS   = len(VOCAB) - 2 # Actual number of words in vocabulary\n",
        "\n",
        "print(\"Vocab length: \", len(VOCAB))\n",
        "print(VOCAB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LA7SapmyXHr7"
      },
      "outputs": [],
      "source": [
        "# Loding the training dataset. Refer to write up section 2 to understand the structure\n",
        "dataset     = np.load('../dataset/wiki.train.npy', allow_pickle=True)\n",
        "\n",
        "# The dataset does not have <sos> and <eos> because they are just regular articles.\n",
        "# TODO: Add <sos> and <eos> to every article in the dataset.\n",
        "# Before doing so, try printing the dataset to see if they are words or integers.\n",
        "articles = []\n",
        "for i in range(dataset.shape[0]):\n",
        "    article = dataset[i]\n",
        "    article = np.insert(article, 0, SOS_TOKEN)\n",
        "    article = np.append(article, EOS_TOKEN)\n",
        "    # articles = np.vstack((articles, article))\n",
        "    articles.append(article)\n",
        "dataset = articles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cell_id": "09f3a2efaeef49ef9f4c0b2b9a614cca",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:12.888156Z",
          "iopub.status.busy": "2022-08-10T14:02:12.884281Z",
          "iopub.status.idle": "2022-08-10T14:02:12.960590Z",
          "shell.execute_reply": "2022-08-10T14:02:12.958805Z",
          "shell.execute_reply.started": "2022-08-10T14:02:12.888058Z"
        },
        "execution_millis": 46,
        "execution_start": 1679856368507,
        "id": "x5znxQhLSwRC",
        "source_hash": "42e4c03c",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation shapes    :  (128, 21) (128,)\n",
            "Test shapes          :  (128, 21)\n"
          ]
        }
      ],
      "source": [
        "# Loading the fixtures for validation and test - prediction\n",
        "fixtures_pred       = np.load('../fixtures/prediction.npz')        # validation\n",
        "fixtures_pred_test  = np.load('../fixtures/prediction_test.npz')   # test\n",
        "\n",
        "print(\"Validation shapes    : \", fixtures_pred['inp'].shape, fixtures_pred['out'].shape)\n",
        "print(\"Test shapes          : \", fixtures_pred_test['inp'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Pes7mCr5WdAw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Gen Shapes          : (128, 31)\n"
          ]
        }
      ],
      "source": [
        "# Loading the test fixtures for generation\n",
        "fixtures_gen_test   = np.load('../fixtures/generation_test.npy')   # test\n",
        "\n",
        "print(\"Test Gen Shapes          :\", fixtures_gen_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jO_Qt7O6rL8L"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((3805,), (4003,))"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example Prediction Dev Input and Output\n",
        "# Optional TODO: You can try printing a few samples from the validation set which has both inputs and outputs\n",
        "dataset[0].shape, dataset[1].shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "aec0165a3f1245dfa52a0cb80dba2578",
        "deepnote_cell_type": "markdown",
        "id": "dHjYhXAOzkrP"
      },
      "source": [
        "# **Dataloader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cell_id": "b2e63a7f6dec4a3f98588725a72a8ff2",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.079390Z",
          "iopub.status.busy": "2022-08-10T14:02:13.078847Z",
          "iopub.status.idle": "2022-08-10T14:02:13.196189Z",
          "shell.execute_reply": "2022-08-10T14:02:13.192167Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.079324Z"
        },
        "execution_millis": 48,
        "execution_start": 1679856368575,
        "id": "OZNrJ8XvSwRF",
        "source_hash": "a81eaa14",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class DataLoaderForLanguageModeling(torch.utils.data.DataLoader): # Inherit from torch.utils.data.DataLoader\n",
        "    \"\"\"\n",
        "        TODO: Define data loader logic here\n",
        "    \"\"\"\n",
        "    # TODO: You can probably add more parameters as well. Eg. sequence length\n",
        "    def __init__(self, dataset, batch_size, sequence_length=3, shuffle= True, drop_last= False):\n",
        "\n",
        "        # If you remember, these are the standard things which you give while defining a dataloader.\n",
        "        # Now you are just customizing your dataloader\n",
        "        self.dataset    = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle    = shuffle\n",
        "        self.drop_last  = drop_last\n",
        "        self.sequence_length = sequence_length\n",
        "        self.dataset_concatenated = np.concatenate(dataset)\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        # What output do you get when you print len(loader)? You get the number of batches\n",
        "        # Your dataset has (579, ) articles and each article has a specified amount of words.\n",
        "        # You concatenate the dataset and then batch parts of it according to the sequence length\n",
        "\n",
        "        total_length = np.sum([self.dataset[i].shape[0] for i in range(len(self.dataset))])\n",
        "        # print(\"checkLen\", total_length)\n",
        "\n",
        "        batch_count = total_length //(self.batch_size*self.sequence_length)\n",
        "        return batch_count\n",
        "\n",
        "    def __iter__(self):\n",
        "        # TODO: Shuffle data if shuffle is True\n",
        "        \n",
        "        if self.shuffle:\n",
        "            # TODO\n",
        "            np.random.shuffle(self.dataset)\n",
        "\n",
        "        # TODO: Set number of batches\n",
        "        num_batches = self.__len__()\n",
        "        \n",
        "        batches = []\n",
        "        \n",
        "        self.dataset_concatenated = np.concatenate(dataset)\n",
        "        \n",
        "        for b in range(num_batches):\n",
        "            batch = self.dataset_concatenated[b*self.batch_size*self.sequence_length:(b+1)*self.batch_size*self.sequence_length+1]\n",
        "            batches.append(batch)\n",
        "            \n",
        "        if self.drop_last:\n",
        "            batches = batches[:-1]\n",
        "            \n",
        "\n",
        "        # TODO: Divide the concetenated dataset into inputs and targets. How do they vary?\n",
        "\n",
        "        # TODO: Reshape the inputs and targets into batches (think about the final shape)\n",
        "\n",
        "        # TODO: Loop though the batches and yield the input and target batch according to the sequence length\n",
        "        batch_idx = 0\n",
        "        \n",
        "        while batch_idx < batches.__len__():\n",
        "            input = batches[batch_idx][:-1].reshape(self.batch_size, self.sequence_length)\n",
        "            target = batches[batch_idx][1:].reshape(self.batch_size, self.sequence_length)\n",
        "            batch_idx += 1\n",
        "            # print(\"checkOutput\", input.shape, target.shape)\n",
        "            yield torch.tensor(input), torch.tensor(target)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cell_id": "773573c8374048d4bcb5a67b905ee2e0",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": false,
        "execution_millis": 3,
        "execution_start": 1679856368714,
        "id": "fBZSzmy10M9M",
        "source_hash": "27952b8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 20]) torch.Size([32, 20])\n",
            "3245\n",
            "x:  ['<sos>', '=', 'Simon', 'Bradstreet', '=', '<eol>', 'Simon', 'Bradstreet', '(', 'baptized', 'March', '18', ',', '1603', '/', '4', 'â€“', 'March', '27', ',']\n",
            "y:  ['=', 'Simon', 'Bradstreet', '=', '<eol>', 'Simon', 'Bradstreet', '(', 'baptized', 'March', '18', ',', '1603', '/', '4', 'â€“', 'March', '27', ',', '1697']\n"
          ]
        }
      ],
      "source": [
        "# Some sanity checks\n",
        "\n",
        "dl = DataLoaderForLanguageModeling(\n",
        "    dataset     = dataset,\n",
        "    batch_size  = 32,\n",
        "    sequence_length = 20,\n",
        "    shuffle     = True,\n",
        "    drop_last   = True,\n",
        "    # Input Extra parameters here if needed\n",
        ")\n",
        "\n",
        "inputs, targets = next(iter(dl))\n",
        "print(inputs.shape, targets.shape)\n",
        "print(dl.__len__())\n",
        "\n",
        "for x, y in dl:\n",
        "    print(\"x: \", [VOCAB[i] for i in x[0, :]])\n",
        "    print(\"y: \", [VOCAB[i] for i in y[0, :]])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "0e75c3c3318d481aa99230d81eb68c13",
        "deepnote_cell_type": "markdown",
        "id": "WcWU0YlnzmVM"
      },
      "source": [
        "# **Language model architecture**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7wwkDXlV3xf"
      },
      "source": [
        "Recurrent network, one-directional or bidirectional, captures certain patterns within a sequence, and can store them into state vector or pass into output. As with convolutional networks, we can build another recurrent layer on top of the first one to capture higher level patterns, build from low-level patterns extracted by the first layer. This leads us to the notion of multi-layer RNN, which consists of two or more recurrent networks, where output of the previous layer is passed to the next layer as input.\n",
        "\n",
        "**Link to PyTorch Documentation**: [LSTM Cell](https://pytorch.org/docs/stable/generated/torch.nn.LSTMCell.html)\n",
        "\n",
        "The following image can be a helpful aid in visualizing the flow of information in a multi-layer RNN with LSTM Cells.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/microsoft/AI-For-Beginners/32043fd2c98de6bbcae857058ac38aaa8140b142/lessons/5-NLP/16-RNN/images/multi-layer-lstm.jpg\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cebwoorWttWe"
      },
      "outputs": [],
      "source": [
        "# Here comes the main portion of this HW.\n",
        "# You can do this with a regular LSTM similar to HW3P2.\n",
        "# However, using LSTMCells presents an opportunity to learn something different\n",
        "\n",
        "class LanguageModel(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, hid_dim): # TODO: Add more parameters as needed\n",
        "        super().__init__()\n",
        "\n",
        "        # For all the layers which you will define, please read the documentation thoroughly before implementation\n",
        "        # TODO: Define a PyTorch embedding layer\n",
        "        self.token_embedding    = torch.nn.Embedding(num_embeddings= vocab_size, embedding_dim = embed_dim)\n",
        "\n",
        "        self.lstm_cells = [\n",
        "            torch.nn.LSTMCell(input_size = embed_dim, hidden_size = hid_dim),\n",
        "            torch.nn.LSTMCell(input_size = hid_dim, hidden_size = hid_dim)\n",
        "        ]\n",
        "\n",
        "        for cell in self.lstm_cells:\n",
        "            cell.apply(self.init_weights)\n",
        "        \n",
        "        self.lstm_cells = torch.nn.Sequential(*self.lstm_cells)\n",
        "\n",
        "        # TODO: Define the parameters\n",
        "        self.token_probability  = torch.nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "        # (Optional) TODO: Weight Tying. You just need to make the embedding layer weights equal to the Linear layer weight.\n",
        "\n",
        "        # So the basic pipline is:\n",
        "        # word -> embedding -> lstm -> projection (linear) to get  probability distribution\n",
        "        # And this is happening across all time steps\n",
        "\n",
        "    def init_weights(self, m):\n",
        "        if type(m) == torch.nn.LSTMCell:\n",
        "            for name, param in m.named_parameters():\n",
        "                if 'weight' in name:\n",
        "                    torch.nn.init.xavier_uniform_(param.data)  # Xavier Uniform initialization\n",
        "                    # torch.nn.init.orthogonal_(param.data)  # Orthogonal initialization\n",
        "                    # torch.nn.init.normal_(param.data, mean=0, std=1)  # Normal initialization\n",
        "                elif 'bias' in name:\n",
        "                    param.data.fill_(0.01)  # Constant initialization\n",
        "    \n",
        "    def rnn_step(self, embedding, hidden_states_list):\n",
        "        next_hidden_states_list = list(hidden_states_list) # Length of the hidden_states_list is same as lstm cells\n",
        "        token_embedding = embedding\n",
        "        for i, lstm in enumerate(self.lstm_cells):\n",
        "            # print(token_embedding.shape, next_hidden_states_list[-1])\n",
        "            hx, cx = lstm.forward(input=token_embedding, hx=next_hidden_states_list[-1])\n",
        "            # print(hx.shape, cx.shape)\n",
        "            # raise EOFError\n",
        "            next_hidden_states_list[i] = (hx, cx)\n",
        "            token_embedding = hx\n",
        "\n",
        "        return token_embedding, next_hidden_states_list\n",
        "\n",
        "    def predict(self, x):\n",
        "        # Refer to Section 1.2.6 to understand this function\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            # TODO: Pass the input sequence through the model\n",
        "            # and return the probability distribution of the last timestep\n",
        "            prob, _ = self.forward(x)\n",
        "            \n",
        "            return prob[:, -1, :]\n",
        "\n",
        "    def generate(self, x, timesteps):\n",
        "        # Refer to section 1.2.4 to understand this function\n",
        "        # Important Note: We do not draw <eos> from the distribution unlike the writeup\n",
        "\n",
        "        timesteps -= 1\n",
        "        if not torch.is_tensor(x):\n",
        "            x = torch.tensor(x).long().to(DEVICE)\n",
        "\n",
        "        # TODO: Pass the input sequence through the model\n",
        "        # Obtain the probability distribution and hidden_states_list of the last timestep\n",
        "        token_prob_dist, hidden_states_list     = self.forward(x)\n",
        "\n",
        "        # TODO: Draw the next predicted token from the probability distribution ()\n",
        "        next_token                              = token_prob_dist.argmax(dim= -1)\n",
        "        # print(timesteps, x.shape, next_token.shape)\n",
        "\n",
        "        # What would generated_sequence be initialized with?\n",
        "        generated_sequence  = [next_token]\n",
        "        with torch.inference_mode():\n",
        "            for t in range(timesteps): # Loop through the timesteps\n",
        "\n",
        "                # TODO: Pass the next_token and hidden_states_list through the model\n",
        "                token_prob_dist, hidden_states_list = self.forward(next_token, hidden_states_list)\n",
        "                # TODO: You will get 2 outputs. What is the shape of the probability distribution?\n",
        "\n",
        "                # TODO: Get the most probable token for the next timestep\n",
        "                next_token = token_prob_dist.argmax(dim= -1)\n",
        "                generated_sequence.append(next_token)\n",
        "                # torch.cuda.empty_cache()\n",
        "\n",
        "            generated_sequence = torch.stack(generated_sequence, dim= 1)[:, -1, :] # keep last timesteps generated words\n",
        "        return generated_sequence\n",
        "\n",
        "    # We are also having a hidden_states_list parameter because you need that in generation\n",
        "    def forward(self, x, hidden_states_list= None): # train model\n",
        "        # x (Batch, Seq_len)\n",
        "        # Note: you dont have to return the sum of log probabilities according to Pseudocode 1 in the writeup\n",
        "        # However, feel free to calculate and print it if you are curious\n",
        "        x = x.long()\n",
        "\n",
        "        batch_size, timesteps   = x.shape\n",
        "\n",
        "        token_prob_distribution = [] # list which will contain probability distributions for all timesteps\n",
        "\n",
        "        # Initializing the hidden hidden_states_list\n",
        "        # Are the elements of the hidden_states_list individual variables or lists of variables themselves?\n",
        "        # Hint: Refer the PyTorch documentation for the answer\n",
        "        hidden_states_list      = [None]*len(self.lstm_cells) if hidden_states_list == None else hidden_states_list\n",
        "\n",
        "        token_embeddings        = self.token_embedding(x) # (Batch, Seq_len, Embed_dim)\n",
        "\n",
        "        # When you get the embeddings of the input x, remember that you get it for all time steps.\n",
        "        # Embedding is just a linear transformation so you can precompute it for all time steps.\n",
        "\n",
        "        for t in range(timesteps): # LSTMCell is for just 1 timestep. Hence you need to loop through the total timesteps\n",
        "\n",
        "            token_embedding_t   = token_embeddings[:, t, :] # (Batch, Embed_dim)\n",
        "\n",
        "            # TODO (What should you do with the hidden_states_list?)\n",
        "            rnn_out, hidden_states_list = self.rnn_step(token_embedding_t, hidden_states_list)\n",
        "\n",
        "            # Map the RNN output to the vocabularyâ€™s dimension and store it in token_prob_dist_t,\n",
        "            # the token probability distribution at time t.\n",
        "            token_prob_dist_t   = self.token_probability(rnn_out)\n",
        "\n",
        "            # Append token_prob_dist_t to a list of token probability distributions.\n",
        "            token_prob_distribution.append(token_prob_dist_t)\n",
        "\n",
        "        # TODO: Stack along the timesteps dimension\n",
        "        token_prob_distribution = torch.stack(token_prob_distribution, dim= 1)\n",
        "\n",
        "        return token_prob_distribution, hidden_states_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DelhoytAQWQa"
      },
      "source": [
        "# **Dataloader, model, loss, optimizer, and scheduler definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jW-YAD9b7FoC"
      },
      "outputs": [],
      "source": [
        "# TODO: Define other hyperparameters\n",
        "\n",
        "config = dict(\n",
        "    batch_size  = 256,\n",
        "    num_epochs  = 30, # 10 to 20 epochs should be enough given the model is good\n",
        "    init_lr     = 0.001,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cell_id": "4aaccf1c32fa480a9a15e8bb8bc4d9e4",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:14.110787Z",
          "iopub.status.busy": "2022-08-10T14:02:14.109778Z",
          "iopub.status.idle": "2022-08-10T14:02:14.929087Z",
          "shell.execute_reply": "2022-08-10T14:02:14.925078Z",
          "shell.execute_reply.started": "2022-08-10T14:02:14.110707Z"
        },
        "id": "DbHH6zXTSwRa",
        "source_hash": "2acff566",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LanguageModel(\n",
            "  (token_embedding): Embedding(33280, 128)\n",
            "  (lstm_cells): Sequential(\n",
            "    (0): LSTMCell(128, 128)\n",
            "    (1): LSTMCell(128, 128)\n",
            "  )\n",
            "  (token_probability): Linear(in_features=128, out_features=33280, bias=True)\n",
            ")\n",
            "torch.Size([256, 20]) torch.Size([256, 20])\n",
            "405\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "LanguageModel                            [256, 20, 33280]          264,192\n",
              "â”œâ”€Embedding: 1-1                         [256, 20, 128]            4,259,840\n",
              "â”œâ”€Linear: 1-2                            [256, 33280]              4,293,120\n",
              "â”œâ”€Linear: 1-3                            [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-4                            [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-5                            [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-6                            [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-7                            [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-8                            [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-9                            [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-10                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-11                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-12                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-13                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-14                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-15                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-16                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-17                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-18                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-19                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-20                           [256, 33280]              (recursive)\n",
              "â”œâ”€Linear: 1-21                           [256, 33280]              (recursive)\n",
              "==========================================================================================\n",
              "Total params: 8,817,152\n",
              "Trainable params: 8,817,152\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 23.07\n",
              "==========================================================================================\n",
              "Input size (MB): 0.04\n",
              "Forward/backward pass size (MB): 1368.39\n",
              "Params size (MB): 34.21\n",
              "Estimated Total Size (MB): 1402.64\n",
              "=========================================================================================="
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torchinfo\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the dataloader\n",
        "loader = DataLoaderForLanguageModeling(dataset, batch_size = config['batch_size'], sequence_length = 20, shuffle = True, drop_last = True)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the model\n",
        "model = LanguageModel(vocab_size = len(VOCAB), embed_dim=128, hid_dim=128).to(DEVICE)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the criterion\n",
        "criterion   = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define the optimizer\n",
        "## Adam/AdamW usually works good for this HW\n",
        "optimizer   = torch.optim.AdamW(model.parameters(), lr = config['init_lr'])\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "# TODO: Define scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.2, patience = 3, verbose = True)\n",
        "\n",
        "# --------------------------------------------------- #\n",
        "\n",
        "#TODO: Define scaler for mixed precision\n",
        "# scaler = NotImplemented\n",
        "\n",
        "print(model)\n",
        "inputs, targets = next(iter(loader))\n",
        "print(inputs.shape, targets.shape)\n",
        "print(loader.__len__())\n",
        "torchinfo.summary(model.to(DEVICE), input_data=inputs.to(DEVICE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "cell_id": "8ed5a6ef54f9446fab752b79c70a0216",
        "deepnote_cell_type": "markdown",
        "id": "TlWF_bpLznup"
      },
      "source": [
        "# **Trainer class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "cell_id": "8ea986fc372643389d1ab4c445659e9d",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.440820Z",
          "iopub.status.busy": "2022-08-10T14:02:13.440281Z",
          "iopub.status.idle": "2022-08-10T14:02:13.644455Z",
          "shell.execute_reply": "2022-08-10T14:02:13.642614Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.440752Z"
        },
        "id": "kIvZOIfjSwRK",
        "source_hash": "451a140f",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Unlike all the P2s, we are using a Trainer class for this HW.\n",
        "# Many researchers also use classes like this for training. You may have encountered them in your project as well.\n",
        "# You dont have to complete everything in this class, you only need to complete the train function.\n",
        "# However, its good to go through the code and see what it does.\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self, model, loader, optimizer, criterion, scheduler, scaler, max_epochs= 1, run_id= 'exp'):\n",
        "        \"\"\"\n",
        "            Use this class to train your model\n",
        "        \"\"\"\n",
        "        # feel free to add any other parameters here\n",
        "        self.model      = model\n",
        "        self.loader     = loader\n",
        "        self.optimizer  = optimizer\n",
        "        self.criterion  = criterion\n",
        "        self.scheduler  = scheduler\n",
        "        self.scaler     = scaler\n",
        "\n",
        "        self.train_losses           = []\n",
        "        self.val_losses             = []\n",
        "        self.prediction_probs       = []\n",
        "        self.prediction_probs_test  = []\n",
        "        self.generated_texts_test   = []\n",
        "        self.epochs                 = 0\n",
        "        self.max_epochs             = max_epochs\n",
        "        self.run_id                 = run_id\n",
        "\n",
        "\n",
        "    def calculate_loss(self, out, target):\n",
        "        # output: (B, T, Vocab_size) - probability distributions\n",
        "        # target: (B, T)\n",
        "        # Read the documentation of CrossEntropyLoss and try to understand how it takes inputs\n",
        "\n",
        "        # Tip: If your target is of shape (B, T) it means that you have B batches with T words.\n",
        "        # Tip: What is the total number of words in this batch?\n",
        "        # Tip: Crossentropy calculates the loss between a label and its probability distribution.\n",
        "\n",
        "        out     = out.reshape(-1, out.shape[-1]) \n",
        "        targets = target.reshape(-1)\n",
        "        loss    = self.criterion(out, targets)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "\n",
        "        self.model.train() # set to training mode\n",
        "        self.model.to(DEVICE)\n",
        "        epoch_loss  = 0\n",
        "        num_batches = 0 # what this for?\n",
        "\n",
        "        for batch_num, (inputs, targets) in enumerate(tqdm(self.loader)):\n",
        "\n",
        "            # TODO: Complete the loop. You should be able to complete this without any helper comments after 3 HWs\n",
        "            # Tip: Use Mixed Precision Training\n",
        "            # For loss calculation, use the calculate_loss function. You need to complete it before using.\n",
        "\n",
        "            inputs = torch.tensor(inputs).long().to(DEVICE)\n",
        "            targets = torch.tensor(targets).long().to(DEVICE)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # with torch.cuda.amp.autocast():\n",
        "            #   # Add code here\n",
        "            #   pass\n",
        "            \n",
        "            out,_ = self.model(inputs)\n",
        "            loss = self.calculate_loss(out, targets)\n",
        "            \n",
        "            loss_item = loss.item()\n",
        "            epoch_loss += loss_item\n",
        "\n",
        "            # TODO: Add backward and, optimiser step and scaler update code here:\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "            # self.scheduler.step(loss)\n",
        "        \n",
        "        epoch_loss = epoch_loss / (batch_num + 1)\n",
        "        self.epochs += 1\n",
        "        print('[TRAIN] \\tEpoch [%d/%d] \\tLoss: %.4f \\tLr: %.6f'\n",
        "                    % (self.epochs, self.max_epochs, epoch_loss, self.optimizer.param_groups[0]['lr']))\n",
        "        self.train_losses.append(epoch_loss)\n",
        "\n",
        "        return (epoch_loss, self.optimizer.param_groups[0]['lr'])\n",
        "\n",
        "\n",
        "\n",
        "    def test(self): # Don't change this function\n",
        "\n",
        "        self.model.eval() # set to eval mode\n",
        "        prediction_probs     = self.model.predict(fixtures_pred['inp']) # get predictions\n",
        "        prediction_probs = prediction_probs.clone().detach().cpu().numpy()\n",
        "        self.prediction_probs.append(prediction_probs)\n",
        "\n",
        "        generated_indexes_test   = self.model.generate(fixtures_gen_test, 10).detach().cpu().numpy() # generated predictions for 10 words\n",
        "\n",
        "        # print(generated_indexes_test.shape)\n",
        "        nll                   = get_prediction_nll(prediction_probs, fixtures_pred['out'])\n",
        "        generated_texts_test  = make_generation_text(fixtures_gen_test, generated_indexes_test, VOCAB)\n",
        "        self.val_losses.append(nll)\n",
        "\n",
        "        self.generated_texts_test.append(generated_texts_test)\n",
        "\n",
        "        # generate predictions for test data\n",
        "        prediction_probs_test = self.model.predict(fixtures_pred_test['inp']).detach().cpu().numpy() # get predictions\n",
        "        self.prediction_probs_test.append(prediction_probs_test)\n",
        "\n",
        "        print('[VAL] \\tEpoch [%d/%d] \\tLoss: %.4f'\n",
        "                      % (self.epochs, self.max_epochs, nll))\n",
        "        return nll\n",
        "\n",
        "\n",
        "    def save(self): # Don't change this function\n",
        "\n",
        "        model_path = os.path.join('../hw4/experiments', self.run_id, 'model-{}.pkl'.format(self.epochs))\n",
        "        torch.save({'state_dict': self.model.state_dict()}, model_path)\n",
        "        np.save(os.path.join('../hw4/experiments', self.run_id, 'prediction-probs-{}.npy'.format(self.epochs)), self.prediction_probs[-1])\n",
        "        np.save(os.path.join('../hw4/experiments', self.run_id, 'prediction-probs-test-{}.npy'.format(self.epochs)), self.prediction_probs_test[-1])\n",
        "\n",
        "        with open(os.path.join('../hw4/experiments', self.run_id, 'generated-texts-{}-test.txt'.format(self.epochs)), 'w') as fw:\n",
        "            fw.write(self.generated_texts_test[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cell_id": "aaff53cf948e44b7b9bd49cbcad0ac58",
        "deepnote_cell_type": "code",
        "deepnote_to_be_reexecuted": true,
        "execution": {
          "iopub.execute_input": "2022-08-10T14:02:13.931258Z",
          "iopub.status.busy": "2022-08-10T14:02:13.930204Z",
          "iopub.status.idle": "2022-08-10T14:02:14.107883Z",
          "shell.execute_reply": "2022-08-10T14:02:14.105987Z",
          "shell.execute_reply.started": "2022-08-10T14:02:13.931185Z"
        },
        "id": "2HCVG5YISwRW",
        "source_hash": "c9f4594a",
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving models, prediction prbabilities, and generated texts to ../hw4/experiments/1713235126\n"
          ]
        }
      ],
      "source": [
        "# Dont change this cell\n",
        "\n",
        "run_id = str(int(time.time()))\n",
        "if not os.path.exists('../hw4/experiments'):\n",
        "    os.mkdir('../hw4/experiments')\n",
        "os.mkdir('../hw4/experiments/%s' % run_id)\n",
        "print(\"Saving models, prediction prbabilities, and generated texts to ../hw4/experiments/%s\" % run_id)\n",
        "\n",
        "# The object of the Trainer class takes in everything\n",
        "trainer = Trainer(\n",
        "    model       = model,\n",
        "    loader      = loader,\n",
        "\n",
        "    optimizer   = optimizer,\n",
        "    criterion   = criterion,\n",
        "    scheduler   = scheduler,\n",
        "    scaler      = None,\n",
        "    max_epochs  = config['num_epochs'],\n",
        "    run_id      = run_id\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfrf1FoSoAI0"
      },
      "source": [
        "# **Wandb**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EuIbwKXToCt-"
      },
      "outputs": [],
      "source": [
        "# # Use wandb? Resume Training?\n",
        "# USE_WANDB = True\n",
        "# RESUME_LOGGING = False\n",
        "\n",
        "# # Create your wandb run\n",
        "\n",
        "# run_name = NotImplemented\n",
        "\n",
        "# if USE_WANDB:\n",
        "\n",
        "#     wandb.login(key=NotImplemented)\n",
        "\n",
        "#     if RESUME_LOGGING:\n",
        "#         run_id = ''\n",
        "#         run = wandb.init(\n",
        "#             id     = run_id, ### Insert specific run id here if you want to resume a previous run\n",
        "#             resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "#             project = \"hw4p1-s24\", ### Project should be created in your wandb account\n",
        "#         )\n",
        "#     else:\n",
        "#         run = wandb.init(\n",
        "#             name    = run_name, ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "#             reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "#             project = \"hw4p1-s24\", ### Project should be created in your wandb account\n",
        "#             config  = config ### Wandb Config for your run\n",
        "#         )\n",
        "\n",
        "#         ### Save your model architecture as a string with str(model)\n",
        "#         model_arch  = str(model)\n",
        "#         ### Save it in a txt file\n",
        "#         arch_file   = open(\"model_arch.txt\", \"w\")\n",
        "#         file_write  = arch_file.write(model_arch)\n",
        "#         arch_file.close()\n",
        "\n",
        "#         ### log it in your wandb run with wandb.save()\n",
        "#         wandb.save('model_arch.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fcxKXL0hrxX"
      },
      "source": [
        "# **Experiments**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# trainer.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-pLh5_LCpVxw"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/405 [00:00<?, ?it/s]/tmp/ipykernel_40639/3181274751.py:58: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = torch.tensor(inputs).long().to(DEVICE)\n",
            "/tmp/ipykernel_40639/3181274751.py:59: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = torch.tensor(targets).long().to(DEVICE)\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 52.72it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [1/30] \tLoss: 7.4309 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [1/30] \tLoss: 6.3990\n",
            "Epoch:  1 NLL:  6.3990116 Best NLL:  1e+30\n",
            "Saving model, prediction probabilities and generated texts for epoch 1 with NLL: 6.3990116\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [2/30] \tLoss: 6.7535 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [2/30] \tLoss: 5.9931\n",
            "Epoch:  2 NLL:  5.9931355 Best NLL:  6.3990116\n",
            "Saving model, prediction probabilities and generated texts for epoch 2 with NLL: 5.9931355\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [3/30] \tLoss: 6.4595 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [3/30] \tLoss: 5.7715\n",
            "Epoch:  3 NLL:  5.7714663 Best NLL:  5.9931355\n",
            "Saving model, prediction probabilities and generated texts for epoch 3 with NLL: 5.7714663\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.71it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [4/30] \tLoss: 6.2603 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [4/30] \tLoss: 5.6455\n",
            "Epoch:  4 NLL:  5.6454744 Best NLL:  5.7714663\n",
            "Saving model, prediction probabilities and generated texts for epoch 4 with NLL: 5.6454744\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [5/30] \tLoss: 6.1037 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [5/30] \tLoss: 5.5522\n",
            "Epoch:  5 NLL:  5.552205 Best NLL:  5.6454744\n",
            "Saving model, prediction probabilities and generated texts for epoch 5 with NLL: 5.552205\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [6/30] \tLoss: 5.9719 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [6/30] \tLoss: 5.4279\n",
            "Epoch:  6 NLL:  5.427873 Best NLL:  5.552205\n",
            "Saving model, prediction probabilities and generated texts for epoch 6 with NLL: 5.427873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [7/30] \tLoss: 5.8680 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [7/30] \tLoss: 5.3444\n",
            "Epoch:  7 NLL:  5.3443546 Best NLL:  5.427873\n",
            "Saving model, prediction probabilities and generated texts for epoch 7 with NLL: 5.3443546\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [8/30] \tLoss: 5.7752 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [8/30] \tLoss: 5.2577\n",
            "Epoch:  8 NLL:  5.257718 Best NLL:  5.3443546\n",
            "Saving model, prediction probabilities and generated texts for epoch 8 with NLL: 5.257718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [9/30] \tLoss: 5.6936 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [9/30] \tLoss: 5.1956\n",
            "Epoch:  9 NLL:  5.195648 Best NLL:  5.257718\n",
            "Saving model, prediction probabilities and generated texts for epoch 9 with NLL: 5.195648\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [10/30] \tLoss: 5.6198 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [10/30] \tLoss: 5.1375\n",
            "Epoch:  10 NLL:  5.137533 Best NLL:  5.195648\n",
            "Saving model, prediction probabilities and generated texts for epoch 10 with NLL: 5.137533\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [11/30] \tLoss: 5.5555 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [11/30] \tLoss: 5.0529\n",
            "Epoch:  11 NLL:  5.0528774 Best NLL:  5.137533\n",
            "Saving model, prediction probabilities and generated texts for epoch 11 with NLL: 5.0528774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.50it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [12/30] \tLoss: 5.4957 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [12/30] \tLoss: 5.0400\n",
            "Epoch:  12 NLL:  5.040036 Best NLL:  5.0528774\n",
            "Saving model, prediction probabilities and generated texts for epoch 12 with NLL: 5.040036\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [13/30] \tLoss: 5.4383 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [13/30] \tLoss: 4.9426\n",
            "Epoch:  13 NLL:  4.942569 Best NLL:  5.040036\n",
            "Saving model, prediction probabilities and generated texts for epoch 13 with NLL: 4.942569\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:08<00:00, 50.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [14/30] \tLoss: 5.3837 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [14/30] \tLoss: 4.9305\n",
            "Epoch:  14 NLL:  4.930463 Best NLL:  4.942569\n",
            "Saving model, prediction probabilities and generated texts for epoch 14 with NLL: 4.930463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 52.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [15/30] \tLoss: 5.3343 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [15/30] \tLoss: 4.9839\n",
            "Epoch:  15 NLL:  4.983934 Best NLL:  4.930463\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [16/30] \tLoss: 5.2877 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [16/30] \tLoss: 4.8743\n",
            "Epoch:  16 NLL:  4.8743153 Best NLL:  4.930463\n",
            "Saving model, prediction probabilities and generated texts for epoch 16 with NLL: 4.8743153\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 52.68it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [17/30] \tLoss: 5.2436 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [17/30] \tLoss: 4.8579\n",
            "Epoch:  17 NLL:  4.857854 Best NLL:  4.8743153\n",
            "Saving model, prediction probabilities and generated texts for epoch 17 with NLL: 4.857854\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [18/30] \tLoss: 5.1994 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [18/30] \tLoss: 4.7638\n",
            "Epoch:  18 NLL:  4.7637873 Best NLL:  4.857854\n",
            "Saving model, prediction probabilities and generated texts for epoch 18 with NLL: 4.7637873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [19/30] \tLoss: 5.1602 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [19/30] \tLoss: 4.7837\n",
            "Epoch:  19 NLL:  4.783743 Best NLL:  4.7637873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.21it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [20/30] \tLoss: 5.1217 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [20/30] \tLoss: 4.8007\n",
            "Epoch:  20 NLL:  4.8007393 Best NLL:  4.7637873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 51.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [21/30] \tLoss: 5.0846 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [21/30] \tLoss: 4.7666\n",
            "Epoch:  21 NLL:  4.7665977 Best NLL:  4.7637873\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 52.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [22/30] \tLoss: 5.0492 \tLr: 0.001000\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [22/30] \tLoss: 4.7980\n",
            "Epoch:  22 NLL:  4.7979765 Best NLL:  4.7637873\n",
            "Epoch 00022: reducing learning rate of group 0 to 2.0000e-04.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 53.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [23/30] \tLoss: 4.9643 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [23/30] \tLoss: 4.7358\n",
            "Epoch:  23 NLL:  4.7358265 Best NLL:  4.7637873\n",
            "Saving model, prediction probabilities and generated texts for epoch 23 with NLL: 4.7358265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 52.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [24/30] \tLoss: 4.9516 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [24/30] \tLoss: 4.7468\n",
            "Epoch:  24 NLL:  4.7468147 Best NLL:  4.7358265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 52.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [25/30] \tLoss: 4.9425 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [25/30] \tLoss: 4.7456\n",
            "Epoch:  25 NLL:  4.74562 Best NLL:  4.7358265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [26/30] \tLoss: 4.9349 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [26/30] \tLoss: 4.7256\n",
            "Epoch:  26 NLL:  4.725598 Best NLL:  4.7358265\n",
            "Saving model, prediction probabilities and generated texts for epoch 26 with NLL: 4.725598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [27/30] \tLoss: 4.9287 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [27/30] \tLoss: 4.7263\n",
            "Epoch:  27 NLL:  4.726281 Best NLL:  4.725598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [28/30] \tLoss: 4.9209 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [28/30] \tLoss: 4.7760\n",
            "Epoch:  28 NLL:  4.775977 Best NLL:  4.725598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [29/30] \tLoss: 4.9137 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [29/30] \tLoss: 4.7604\n",
            "Epoch:  29 NLL:  4.760352 Best NLL:  4.725598\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 404/405 [00:07<00:00, 54.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[TRAIN] \tEpoch [30/30] \tLoss: 4.9068 \tLr: 0.000200\n",
            "[   72 24820 21959    79  1419 25821 13052  3909  1419  2118 31190 24567\n",
            " 32747  6085 26516  1419 31353 29478 22356 14658  1420 13710 22534  1512\n",
            " 25821 31353 26874 25821 29310  1794 16004 31293 21626    76  1424     1\n",
            " 25177  1311    76 31352 31353    76  2858  1417 23318   373     1 15111\n",
            " 31353    76    79 19085 23592    76 12959     1 25871 25821 22633  1423\n",
            " 31589  1419 22845 18261  1424 31353 32873 19623 31353 16077  1424 26375\n",
            " 31352 27417 32883 17135 31429    79 31994 31673 25525 22968 31353 13276\n",
            " 25821 31514  1424 14658  1417 11475 31994 29707 25821 26753 22968 33248\n",
            " 17731 32209 16672 24098 25410 31353 27904 28296 10120    79 31353  1417\n",
            " 15898    76 15340 15577  1420 14247 31205  1419 32883    79 19652   869\n",
            " 14118 25639 15310 15391 31353 29294  5172  8881]\n",
            "[VAL] \tEpoch [30/30] \tLoss: 4.7307\n",
            "Epoch:  30 NLL:  4.730708 Best NLL:  4.725598\n",
            "Epoch 00030: reducing learning rate of group 0 to 4.0000e-05.\n"
          ]
        }
      ],
      "source": [
        "# Run the experiments loop.\n",
        "# Each epoch wont take more than 2-3min. If its taking more time, it might be due to (but not limited to) the following:\n",
        "#   * You might be overlapping batches\n",
        "#       Eg. Input: \"I had biryani for lunch today\" and sequence length = 3,\n",
        "#           --> \"I had biryani\", \"for lunch today\" are ideal examples for inputs\n",
        "#           --> \"I had biryani\", \"had biryani for\", \"biryani for lunch\", ... is just redundant info :')\n",
        "#   * Your length calculation in the dataloader might be wrong\n",
        "# If you haven't had biryani, try it :D\n",
        "\n",
        "# wandb.watch(model, log=\"all\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# %%time\n",
        "best_nll = 1e30\n",
        "for epoch in range(config['num_epochs']):\n",
        "    train_loss, curr_lr = trainer.train()\n",
        "    nll = trainer.test()\n",
        "    print(\"Epoch: \", epoch+1, \"NLL: \", nll, \"Best NLL: \", best_nll)\n",
        "    scheduler.step(nll)\n",
        "    if nll < best_nll:\n",
        "        best_nll = nll\n",
        "        print(\"Saving model, prediction probabilities and generated texts for epoch \"+str(epoch+1)+\" with NLL: \"+ str(best_nll))\n",
        "        trainer.save()\n",
        "\n",
        "    # wandb.log({\"train_loss\":train_loss,\n",
        "    #            \"nll\": nll,\n",
        "    #            \"learning_rate\": curr_lr\n",
        "    #           })\n",
        "\n",
        "### Finish your wandb run\n",
        "# run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "5Oq8tLAo7jF_"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "451"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "i_gYqXq9Jgo1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure()\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.train_losses, label='Training losses')\n",
        "plt.plot(range(1, trainer.epochs + 1), trainer.val_losses, label='Validation losses')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('NLL')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vleoDhQe7jF_"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbDQm49BKrdd"
      },
      "source": [
        " - Now that you have trained your model and got satisfactory validation NLL on the single token prediction task, you can evaluate the generations you created too\n",
        " - We will use the perplexity metric to evaluate generations using a large language model available through the OpenAI API. Read the handout for instructions on how to sign up for the API and obtain and API key.\n",
        " - Once you add credits to your account, run this cell to get the perplexity.\n",
        " - You will submit this perplexity value for grading the generation component of this homework.\n",
        " - A perplexity of under **1400** will give you full credit on the generation part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvVGQNzUL3IK"
      },
      "source": [
        "Change only the **submission_run_id**, **submission_epoch**, and **api_key** in the following cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qqJ9lf7W7jF_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/128 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 128/128 [00:36<00:00,  3.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your mean perplexity for generated sequences: 445.90097113231025\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# DO NOT CHANGE THE CODE IN THIS CELL EXCEPT submission_run_id, submission_epoch, AND api_key\n",
        "# PLEASE BE HONEST IN REPORTING THE PERPLEXITY VALUE!\n",
        "# WE WILL RANDOMLY CHECK SOME SUBMISSIONS USING THE SAME CODE AS THIS AND A BIG DIFFERENCE IN PERPLEXITY WILL RESULT IN AN AIV.\n",
        "\n",
        "import openai\n",
        "import json\n",
        "\n",
        "openai.api_key = json.load(open(\"../../../safestore.json\")).get(\"openai_api_key\")\n",
        "\n",
        "\n",
        "# Add you submission_run_id and submission_epoch here --------------------------------------------------\n",
        "# Fill the run id and epoch number to be used for submission.\n",
        "# You will use the same run id and epoch number to generate the handin.\n",
        "\n",
        "submission_run_id = run_id # TODO\n",
        "submission_epoch = 26 # TODO\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "n_tests = 128\n",
        "\n",
        "with open(os.path.join('./experiments', submission_run_id, 'generated-texts-{}-test.txt'.format(submission_epoch)), 'r', encoding='utf-8') as f:\n",
        "    generated = list(f)\n",
        "\n",
        "assert len(generated) == n_tests\n",
        "for item in generated:\n",
        "    assert type(item) is str\n",
        "\n",
        "parsed_generated = []\n",
        "\n",
        "for text in generated:\n",
        "    start_index = text.index(\"<sos>\")\n",
        "    temp = text[start_index+6:]\n",
        "    generation_start_index = temp.index(\"| \")\n",
        "    parsed_text = temp[:generation_start_index] + temp[generation_start_index+2:]\n",
        "    parsed_text = parsed_text.replace(\"<eol>\", \"\\n\")\n",
        "    parsed_generated.append(parsed_text)\n",
        "\n",
        "def perplexity(text, modelname):\n",
        "    \"\"\"Compute the perplexity of the provided text.\"\"\"\n",
        "    completion = openai.Completion.create(\n",
        "        model=modelname,\n",
        "        prompt=text,\n",
        "        logprobs=0,\n",
        "        max_tokens=0,\n",
        "        temperature=1.0,\n",
        "        echo=True)\n",
        "    token_logprobs = completion['choices'][0]['logprobs']['token_logprobs']\n",
        "    ll = np.mean([i for i in token_logprobs if i is not None])\n",
        "    ppl = np.exp(-ll)\n",
        "    return ppl\n",
        "\n",
        "# Add you API key here --------------------------------------------------\n",
        "# However, delete the key from the notebook before creating the handin.\n",
        "# REMEMBER: ALWAYS KEEP YOUR API KEYS AND SECRETS SECURE.\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "\n",
        "modelname = 'text-embedding-ada-002'\n",
        "\n",
        "perps = [perplexity(text, modelname) for text in tqdm(parsed_generated)]\n",
        "avg_perp = np.mean(perps)\n",
        "\n",
        "# Report this number when running the makefile to create the handin\n",
        "print(\"Your mean perplexity for generated sequences: {}\".format(avg_perp))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-g3iGbFGXjC3"
      },
      "source": [
        "**NOTE**:\n",
        "- If you get a \"The server is overloaded or not ready yet\" when trying to run the above cell, simply try re-running after some time.\n",
        "- You will need to add credits ($5) to your open-ai account to get rid of the limit error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBqjqy-EyU27"
      },
      "source": [
        "# **Submission**\n",
        "Navigate to the handout directory to run the below cell. This command will create the handin with all the required files (including attention.py). So make sure you have the entire handout directory wherever you are running this notebook (local machine, Colab, AWS, etc.).\n",
        "\n",
        "**IMPORTANT NOTE:** This command requires that this c**ompleted notebook be in the hw4 folder inside the handout directory**. If you are on colab, this notebook you are working on **DOES NOT** live in the handout directory. You must **download it and then upload it** to the hw4 folder replacing the empty starter notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hWRyPvWmgLQs"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "echo 445.90097113231025 > generation_ppl.txt\n",
            "cp hw4/experiments/1713235126/prediction-probs-test-26.npy prediction_probs.npy\n",
            "cp hw4/experiments/1713235126/generated-texts-26-test.txt generated_texts.txt\n",
            "cp hw4/hw4p1.ipynb training.ipynb\n",
            "cp hw4/attention.py attention.py\n",
            "tar -cvzf handin.tar training.ipynb prediction_probs.npy generated_texts.txt generation_ppl.txt attention.py\n",
            "training.ipynb\n",
            "prediction_probs.npy\n",
            "generated_texts.txt\n",
            "generation_ppl.txt\n",
            "attention.py\n",
            "rm -f generated_texts.txt prediction_probs.npy training.ipynb generation_ppl.txt attention.py\n"
          ]
        }
      ],
      "source": [
        "# TODO: Generate the handin to submit to autolab\n",
        "\n",
        "# For example:\n",
        "# !make runid=1705009752 epoch=9 ppl=1287.0752467922216\n",
        "\n",
        "!cd .. && make runid=1713235126 epoch=26 ppl=445.90097113231025"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THcllS3fnFR3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "EB2bOV3bzYLR",
        "WcWU0YlnzmVM",
        "DelhoytAQWQa",
        "TlWF_bpLznup",
        "Dfrf1FoSoAI0",
        "0fcxKXL0hrxX",
        "vleoDhQe7jF_",
        "BBqjqy-EyU27"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "deepnote": {},
    "deepnote_execution_queue": [],
    "deepnote_notebook_id": "989a3c3836794109ac641230122845a3",
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
